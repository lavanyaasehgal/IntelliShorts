{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNw3qJGP68fw6EonKyeSVQq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavanyaasehgal/IntelliShorts/blob/main/NLP_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "tVUgxTBSIkOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "YhD7iPwLIaLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUPbXFTtIOwR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from string import punctuation\n",
        "import string, re, nltk\n",
        "nltk.download(\"all\")\n",
        "!python -m spacy download en_core_web_sm\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "import contractions\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.optimizers import AdamW\n",
        "from keras.utils import to_categorical\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "from transformers import TFAutoModelForSeq2SeqLM, AutoModelForSeq2SeqLM,DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from datasets import load_metric, load_dataset\n",
        "import accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzCpmgT2HZeu"
      },
      "source": [
        "# Cleaning (functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKTurH6tHb0A"
      },
      "outputs": [],
      "source": [
        "#Regular Expression\n",
        "regexp = RegexpTokenizer(\"[\\w']+\")\n",
        "#Lowercase\n",
        "def text_lower(text):\n",
        "  text = text.lower()\n",
        "  return text\n",
        "#Remove Whitespace\n",
        "def remove_whitespace(text):\n",
        "  text = text.strip()\n",
        "  return text\n",
        "#Remove Punctuation\n",
        "def remove_punctuation(text):\n",
        "  punct = string.punctuation\n",
        "  punct = punct.replace(\"'\",\"\")\n",
        "  text = text.translate(str.maketrans(\"\", \"\",punct))\n",
        "  return text\n",
        "#Remove HTML\n",
        "def remove_html(text):\n",
        "  html = re.compile(r'<.*?>')\n",
        "  text = html.sub(r'',text)\n",
        "  return text\n",
        "# Removing emojis\n",
        "def remove_emoji(text):\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "    u\"\\U00002702-\\U000027B0\"\n",
        "    u\"\\U000024C2-\\U0001F251\"\n",
        "    \"]+\",flags=re.UNICODE\n",
        "  )\n",
        "  text = emoji_pattern.sub(r'',text)\n",
        "  return text\n",
        "#Remove HTML\n",
        "def remove_html(text):\n",
        "  html = re.compile(r'<.*?>')\n",
        "  text = html.sub(r'',text)\n",
        "  return text\n",
        "#Remove URLS\n",
        "def remove_http_links(text):\n",
        "  text = re.sub('http://\\S+|https://\\S+','',text)\n",
        "  return text\n",
        "#Convert Contractions like you're\n",
        "def convert_contractions(text):\n",
        "  text = contractions.fix(text)\n",
        "  return text\n",
        "#Remove Stopwords\n",
        "def remove_stopwords(text):\n",
        "  text = \" \".join([word for word in nltk.tokenize.word_tokenize(text) if word not in stopwords.words('english')])\n",
        "  return text\n",
        "# Lemmatization\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable = ['parser', 'ner'])\n",
        "\n",
        "def lemmatize(text):\n",
        "  text = \" \".join([token.lemma_ for token in nlp(text)])\n",
        "  return text\n",
        "#Remove Non-Alphabetic Characters\n",
        "def discard_non_alpha(text):\n",
        "  word_list_non_alpha = [word for word in regexp.tokenize(text) if word.isalpha()]\n",
        "  text = \" \".join(word_list_non_alpha)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  text = text_lower(text)\n",
        "  text = remove_whitespace(text)\n",
        "  text = re.sub('\\n' , '', text)\n",
        "  text = re.sub('\\[.*?\\]', '', text)\n",
        "  text = remove_http_links(text)\n",
        "  text = remove_punctuation(text)\n",
        "  text = remove_html(text)\n",
        "  text = remove_emoji(text)\n",
        "  text = convert_contractions(text)\n",
        "  text = remove_stopwords(text)\n",
        "  text = discard_non_alpha(text)\n",
        "  text = lemmatize(text)\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "kPRXICjVladh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}