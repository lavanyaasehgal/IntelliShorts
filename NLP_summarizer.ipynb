{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "tVUgxTBSIkOU",
        "aVkWbamVJ2Zu",
        "SzCpmgT2HZeu",
        "Y1SlSprOUl8P",
        "HqRSVhh0UY2G",
        "LMK4c2nZpr7l",
        "oG1-_KJZ6lUb"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "81a57efa95124a6daacb36bd4fe92252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a4a92ada78554a4ab06cf25350c39cf9",
              "IPY_MODEL_323337bd0f704081bd592b32bd1824ef",
              "IPY_MODEL_d6686c3f47914b23bb3acc8cc14471ba"
            ],
            "layout": "IPY_MODEL_6cef58a813fb4620b17a68bd53a094fe"
          }
        },
        "a4a92ada78554a4ab06cf25350c39cf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20e5cd3fd9914e7fa06e5216e42a6d62",
            "placeholder": "​",
            "style": "IPY_MODEL_eaf8406d929f4676b3f62ed4b1814aa5",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "323337bd0f704081bd592b32bd1824ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e951a926110a46af838a52ee502221b7",
            "max": 2324,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb0feeef614e429395d912c353a5ed22",
            "value": 2324
          }
        },
        "d6686c3f47914b23bb3acc8cc14471ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_084817913fdf4cfc94bc50f60cca81e2",
            "placeholder": "​",
            "style": "IPY_MODEL_6790bab457094e89a4e0fa7f58388ad6",
            "value": " 2.32k/2.32k [00:00&lt;00:00, 134kB/s]"
          }
        },
        "6cef58a813fb4620b17a68bd53a094fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20e5cd3fd9914e7fa06e5216e42a6d62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eaf8406d929f4676b3f62ed4b1814aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e951a926110a46af838a52ee502221b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb0feeef614e429395d912c353a5ed22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "084817913fdf4cfc94bc50f60cca81e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6790bab457094e89a4e0fa7f58388ad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ede49250f1e4606acd6e086d8988cc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d728745d27049a7bcf406592b74d5a4",
              "IPY_MODEL_6f6a00a781354969b9493ab172da645e",
              "IPY_MODEL_af06185bb460494d8faed3b719f8af74"
            ],
            "layout": "IPY_MODEL_87a037e769594c7e9b8deb88b3b7e6e6"
          }
        },
        "8d728745d27049a7bcf406592b74d5a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_553b285cef0b4fb7b1d8c12e82c4c48a",
            "placeholder": "​",
            "style": "IPY_MODEL_bac1a40c658e4728b774c1cb7a0758b4",
            "value": "Downloading (…)ve/main/spiece.model: 100%"
          }
        },
        "6f6a00a781354969b9493ab172da645e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a0df6b3ff1040a782312639508c4e65",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_083530e04d954865a98166b2e829f583",
            "value": 791656
          }
        },
        "af06185bb460494d8faed3b719f8af74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c2ca2dc11a2442e94b8161d762c0b36",
            "placeholder": "​",
            "style": "IPY_MODEL_0d65d981545a4537b411902683f90ebb",
            "value": " 792k/792k [00:00&lt;00:00, 3.01MB/s]"
          }
        },
        "87a037e769594c7e9b8deb88b3b7e6e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "553b285cef0b4fb7b1d8c12e82c4c48a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bac1a40c658e4728b774c1cb7a0758b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a0df6b3ff1040a782312639508c4e65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "083530e04d954865a98166b2e829f583": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c2ca2dc11a2442e94b8161d762c0b36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d65d981545a4537b411902683f90ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5227e3d4eb5645c3b9606368739db7b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d883570a5b34fd0a5746d496135597d",
              "IPY_MODEL_417f1b1409ab49538045114b1a98e66b",
              "IPY_MODEL_518cfb1289f94a6485033122b0a791e7"
            ],
            "layout": "IPY_MODEL_ad820011ffff4d14ab1e3bd068c3e860"
          }
        },
        "4d883570a5b34fd0a5746d496135597d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ecb10e2d66d43b9891666f9dea0ae53",
            "placeholder": "​",
            "style": "IPY_MODEL_3b1e2a2f44ae4345a52c9f7f16d584a3",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "417f1b1409ab49538045114b1a98e66b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26eef67c5d7a4f5b93750f0e5342a7c1",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74bad1b8610947af87bd16df5bebeae7",
            "value": 1389353
          }
        },
        "518cfb1289f94a6485033122b0a791e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d39c48c3fe5a4d1d9a8becda8c921c80",
            "placeholder": "​",
            "style": "IPY_MODEL_4315997378244fdd919efbea51b136a0",
            "value": " 1.39M/1.39M [00:00&lt;00:00, 19.3MB/s]"
          }
        },
        "ad820011ffff4d14ab1e3bd068c3e860": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ecb10e2d66d43b9891666f9dea0ae53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b1e2a2f44ae4345a52c9f7f16d584a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26eef67c5d7a4f5b93750f0e5342a7c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74bad1b8610947af87bd16df5bebeae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d39c48c3fe5a4d1d9a8becda8c921c80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4315997378244fdd919efbea51b136a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lavanyaasehgal/IntelliShorts/blob/main/NLP_summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "tVUgxTBSIkOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcDbJyrtjOq9",
        "outputId": "d89bb5cd-1f60-44ae-e264-62e222c83590"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZu65oRcD9U_",
        "outputId": "7abfca4e-544c-458a-b0c9-db81ab56f86e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (1.23.5)\n",
            "Collecting boto3 (from pytorch-pretrained-bert)\n",
            "  Downloading boto3-1.28.73-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (4.66.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch-pretrained-bert) (2023.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (2.1.0)\n",
            "Collecting botocore<1.32.0,>=1.31.73 (from boto3->pytorch-pretrained-bert)\n",
            "  Downloading botocore-1.31.73-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch-pretrained-bert)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3->pytorch-pretrained-bert)\n",
            "  Downloading s3transfer-0.7.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch-pretrained-bert) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.73->boto3->pytorch-pretrained-bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch-pretrained-bert) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch-pretrained-bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.73->boto3->pytorch-pretrained-bert) (1.16.0)\n",
            "Downloading boto3-1.28.73-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.31.73-py3-none-any.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.28.73 botocore-1.31.73 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.7.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Flask -q\n",
        "!pip install contractions -q\n",
        "!pip install -U transformers -q\n",
        "!pip install --upgrade transformers\n",
        "!pip install -U accelerate -q\n",
        "!pip install keras_nlp -q\n",
        "!pip install datasets -q\n",
        "!pip install huggingface-hub -q\n",
        "!pip install rouge-score -q\n",
        "!pip install sentencepiece -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWtf0YtHKtCL",
        "outputId": "307dda68-ca69-4859-cf32-94f656514d7e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gnews -q\n",
        "# !pip install GoogleNews -q\n",
        "# !pip install fake_useragent -q\n",
        "# !pip install newspaper3k -q\n",
        "# !pip install newsapi-python -q\n",
        "# !pip install pycountry -q\n",
        "# !pip install gnewsclient -q"
      ],
      "metadata": {
        "id": "JV5rV0W1K2Ft"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random"
      ],
      "metadata": {
        "id": "YhD7iPwLIaLS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade pip\n",
        "# !pip install --upgrade setuptools"
      ],
      "metadata": {
        "id": "v1tG2NtsDXej"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUPbXFTtIOwR"
      },
      "outputs": [],
      "source": [
        "import string, re, nltk\n",
        "# nltk.download(\"all\")\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from string import punctuation\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed,Dropout\n",
        "from keras.optimizers import AdamW\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import warnings\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import accelerate\n",
        "import spacy\n",
        "import contractions\n",
        "import torch\n",
        "import gensim\n",
        "from bs4 import BeautifulSoup\n",
        "from datasets import load_metric, load_dataset\n",
        "\n",
        "# import pytorch\n",
        "\n",
        "import transformers\n",
        "# from pytorch_pretrained_bert.modeling import BertPreTrainedModel,BertForSummarization, BertTokenizer, BertModel\n",
        "# from pytorch_pretrained_bert import\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer,AutoTokenizer, TFAutoModelForSeq2SeqLM, AutoModelForSeq2SeqLM,DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "\n",
        "import sentencepiece as spm\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing data"
      ],
      "metadata": {
        "id": "aVkWbamVJ2Zu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ai=pd.read_csv('/content/drive/MyDrive/nlp data/ai_articles.csv')\n",
        "# ml=pd.read_csv('/content/drive/MyDrive/nlp data/ml_articles.csv')\n",
        "# explainable_ai=pd.read_csv('/content/drive/MyDrive/nlp data/explainable_articles.csv')\n",
        "# deep_learning=pd.read_csv('/content/drive/MyDrive/nlp data/dl_articles.csv')\n",
        "# data_mining=pd.read_csv('/content/drive/MyDrive/nlp data/dm_articles.csv')\n",
        "# gpt_art=pd.read_csv('/content/drive/MyDrive/nlp data/gpt_articles.csv')\n",
        "# nlp=pd.read_csv('/content/drive/MyDrive/nlp data/nlp_articles.csv')\n",
        "# reinforcement_learning=pd.read_csv('/content/drive/MyDrive/nlp data/rl_articles.csv')\n",
        "# transfer_leaning=pd.read_csv('/content/drive/MyDrive/nlp data/transfer_articles.csv')"
      ],
      "metadata": {
        "id": "BSOQgt7AJ4Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_unclean = pd.DataFrame()\n",
        "# file_paths=[ai,ml,explainable_ai,deep_learning,data_mining,gpt_art,nlp,reinforcement_learning,transfer_leaning]\n",
        "# # Iterate through the files and combine them\n",
        "# for file in file_paths:\n",
        "#     combined_unclean = combined_unclean.append(file, ignore_index=True)\n",
        "\n",
        "# # Save the combined data to a new CSV file\n",
        "# combined_unclean.to_csv('/content/drive/MyDrive/nlp data/combined_unclean.csv', index=False)"
      ],
      "metadata": {
        "id": "_LKyOVXGKA_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_unclean=pd.read_csv('/content/drive/MyDrive/nlp data/combined_unclean.csv')"
      ],
      "metadata": {
        "id": "si_D5rBvNFWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzCpmgT2HZeu"
      },
      "source": [
        "# Cleaning (functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RKTurH6tHb0A"
      },
      "outputs": [],
      "source": [
        "# #Regular Expression\n",
        "# regexp = RegexpTokenizer(\"[\\w']+\")\n",
        "# #Lowercase\n",
        "# def text_lower(text):\n",
        "#   text = text.lower()\n",
        "#   return text\n",
        "# #Remove Whitespace\n",
        "# def remove_whitespace(text):\n",
        "#   text = text.strip()\n",
        "#   return text\n",
        "# #Remove Punctuation\n",
        "# def remove_punctuation(text):\n",
        "#   punct = string.punctuation\n",
        "#   punct = punct.replace(\"'\",\"\")\n",
        "#   text = text.translate(str.maketrans(\"\", \"\",punct))\n",
        "#   return text\n",
        "# #Remove HTML\n",
        "# def remove_html(text):\n",
        "#   html = re.compile(r'<.*?>')\n",
        "#   text = html.sub(r'',text)\n",
        "#   return text\n",
        "# # Removing emojis\n",
        "# def remove_emoji(text):\n",
        "#   emoji_pattern = re.compile(\"[\"\n",
        "#     u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "#     u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "#     u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "#     u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
        "#     u\"\\U00002702-\\U000027B0\"\n",
        "#     u\"\\U000024C2-\\U0001F251\"\n",
        "#     \"]+\",flags=re.UNICODE\n",
        "#   )\n",
        "#   text = emoji_pattern.sub(r'',text)\n",
        "#   return text\n",
        "# #Remove HTML\n",
        "# def remove_html(text):\n",
        "#   html = re.compile(r'<.*?>')\n",
        "#   text = html.sub(r'',text)\n",
        "#   return text\n",
        "# #Remove URLS\n",
        "# def remove_http_links(text):\n",
        "#   text = re.sub('http://\\S+|https://\\S+','',text)\n",
        "#   return text\n",
        "# #Convert Contractions like you're\n",
        "# def convert_contractions(text):\n",
        "#   text = contractions.fix(text)\n",
        "#   return text\n",
        "# #Remove Stopwords\n",
        "# def remove_stopwords(text):\n",
        "#   text = \" \".join([word for word in nltk.tokenize.word_tokenize(text) if word not in stopwords.words('english')])\n",
        "#   return text\n",
        "# # Lemmatization\n",
        "# nlp = spacy.load(\"en_core_web_sm\", disable = ['parser', 'ner'])\n",
        "\n",
        "# def lemmatize(text):\n",
        "#   text = \" \".join([token.lemma_ for token in nlp(text)])\n",
        "#   return text\n",
        "# #Remove Non-Alphabetic Characters\n",
        "# def discard_non_alpha(text):\n",
        "#   word_list_non_alpha = [word for word in regexp.tokenize(text) if word.isalpha()]\n",
        "#   text = \" \".join(word_list_non_alpha)\n",
        "#   return text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess_text(text):\n",
        "#   text = text_lower(text)\n",
        "#   text = remove_whitespace(text)\n",
        "#   text = re.sub('\\n' , '', text)\n",
        "#   text = re.sub('\\[.*?\\]', '', text)\n",
        "#   text = remove_http_links(text)\n",
        "#   text = remove_punctuation(text)\n",
        "#   text = remove_html(text)\n",
        "#   text = remove_emoji(text)\n",
        "#   text = convert_contractions(text)\n",
        "#   text = remove_stopwords(text)\n",
        "#   text = discard_non_alpha(text)\n",
        "#   # text = lemmatize(text)\n",
        "\n",
        "#   return text"
      ],
      "metadata": {
        "id": "kPRXICjVladh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_data=[]\n",
        "# combined_data=combined_unclean"
      ],
      "metadata": {
        "id": "5hf2Pc888rdY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_data['content'] = combined_data['content'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "hine22Uih4Yc"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_data.isna().sum()"
      ],
      "metadata": {
        "id": "74O8MuvM1NYq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_data = combined_data.dropna(subset=['title'])"
      ],
      "metadata": {
        "id": "CFF48Y_D1YKv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_data.isna().sum()"
      ],
      "metadata": {
        "id": "2X5HkIvxUSMQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined_data.to_csv('/content/drive/MyDrive/nlp data/combined_data.csv', index=False)"
      ],
      "metadata": {
        "id": "Xaf2egiQnrIk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# articles_to_summ = combined_data[['title', 'content']]"
      ],
      "metadata": {
        "id": "aGTF03-KkhmE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data=pd.read_csv('/content/drive/MyDrive/nlp data/combined_data.csv')"
      ],
      "metadata": {
        "id": "4BpVtJMvILQb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnt=0\n",
        "for i in combined_data['title']:\n",
        "    if(len(i.split())<=15):\n",
        "        cnt=cnt+1\n",
        "print(cnt/len(combined_data['title']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQK-2oU_jGCi",
        "outputId": "c81c1645-58cd-4c82-8daf-4022bbf15d16"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8384332925336597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnt=0\n",
        "for i in combined_data['content']:\n",
        "    if(len(i.split())<=50):\n",
        "        cnt=cnt+1\n",
        "print(cnt/len(combined_data['content']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnMUMLYOy02r",
        "outputId": "1e9ede53-247c-4d78-f346-edca8013011a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.042839657282741736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPU"
      ],
      "metadata": {
        "id": "Y1SlSprOUl8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available and being used\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU is not available, using CPU instead\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KU3v2GU637m",
        "outputId": "d0b36f18-d384-449a-e9b7-bcde743ce758"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is not available, using CPU instead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "3tBNwtLC9a0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "LgWuP68ak05N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = train_test_split(combined_data, train_size=0.7, random_state=42)"
      ],
      "metadata": {
        "id": "oB1fBRsXk3Xu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data[['title', 'content']]\n",
        "test_data = test_data[['title', 'content']]"
      ],
      "metadata": {
        "id": "t9m2gSRBql7E"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = train_data.reset_index(drop=True)\n",
        "test = test_data.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "K5eVjiC4r18K"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tr,x_val,y_tr,y_val=train_test_split(np.array(combined_data['title']),np.array(combined_data['content']),test_size=0.2,random_state=0,shuffle=True)"
      ],
      "metadata": {
        "id": "HqjLEni2zLIQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT"
      ],
      "metadata": {
        "id": "Q0BczBM4KtjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Load a pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Add a pad token and set it as the EOS token\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]', 'eos_token': '[PAD]'})\n",
        "model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load your dataset (you need to define and load your dataset here)\n",
        "# Assuming you have a DataFrame 'train' for training data and 'test' for test data\n",
        "# with 'content' column for both.\n",
        "\n",
        "# Preprocess your data (tokenize, encode, etc.) - customize this based on your dataset structure\n",
        "train_texts = train['content'].tolist()\n",
        "train_summaries = train['title'].tolist()\n",
        "\n",
        "# Initialize a list to store the generated summaries\n",
        "generated_summaries = []\n",
        "\n",
        "# Define the maximum chunk length (less than 1024 tokens)\n",
        "max_chunk_length = 700  # Adjust this as needed\n",
        "\n",
        "for text in train_texts:\n",
        "    # Split the long input text into smaller chunks\n",
        "    chunks = [text[i:i+max_chunk_length] for i in range(0, len(text), max_chunk_length)]\n",
        "\n",
        "    for chunk in chunks:\n",
        "        # Tokenize and encode the chunk\n",
        "        input_ids = tokenizer.encode(chunk, return_tensors=\"pt\", max_length=len(chunk), truncation=True, padding=True)\n",
        "\n",
        "        # Generate summaries for each chunk\n",
        "        summary_ids = model.generate(input_ids, max_length=len(chunk), min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True, pad_token_id=model.config.pad_token_id, attention_mask=input_ids.ne(model.config.pad_token_id))\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        generated_summaries.append(summary)\n",
        "\n",
        "# Now, 'generated_summaries' contains the summaries for the training data\n",
        "\n",
        "# Initialize a list to store the generated summaries for the test data\n",
        "test_texts = test['content'].tolist()\n",
        "test_generated_summaries = []\n",
        "\n",
        "for text in test_texts:\n",
        "    # Split the long input text into smaller chunks\n",
        "    chunks = [text[i:i+max_chunk_length] for i in range(0, len(text), max_chunk_length)]\n",
        "\n",
        "    for chunk in chunks:\n",
        "        # Tokenize and encode the chunk\n",
        "        input_ids = tokenizer.encode(chunk, return_tensors=\"pt\", max_length=len(chunk), truncation=True, padding=True)\n",
        "\n",
        "        # Generate summaries for each chunk in the test data\n",
        "        summary_ids = model.generate(input_ids, max_length=len(chunk), min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True, pad_token_id=model.config.pad_token_id, attention_mask=input_ids.ne(model.config.pad_token_id))\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        test_generated_summaries.append(summary)\n",
        "\n",
        "# Now, 'test_generated_summaries' contains the summaries for the test data\n"
      ],
      "metadata": {
        "id": "iK9xETQ-MIJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTSUM"
      ],
      "metadata": {
        "id": "9NbPLf1KMVyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame()\n",
        "df=combined_data[['title','content']]"
      ],
      "metadata": {
        "id": "R4A09YVfMenC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained BERT model and tokenizer for summarization\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = BertForSummarization.from_pretrained(model_name)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Assuming you have a DataFrame called 'df' with a 'content' column\n",
        "df['summary'] = \"\"\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    content = row['content']\n",
        "    sentences = nltk.sent_tokenize(content)  # Tokenize into sentences\n",
        "    scores = []  # Store importance scores for each sentence\n",
        "    for sentence in sentences:\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        outputs = model(**inputs)\n",
        "        scores.append(outputs.logits.sum().item())\n",
        "    top_sentences = [sentences[i] for i in sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:5]]  # Select the top 5 sentences\n",
        "    summary = \" \".join(top_sentences)  # Combine the selected sentences as the summary\n",
        "    df.at[index, 'summary'] = summary\n",
        "\n",
        "# Save the DataFrame with the generated summaries\n",
        "df.to_csv('summarized_data_bert.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "jf53VeOHMZCP",
        "outputId": "e48aece0-ad98-45a9-9a2a-36be1f0f32a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-6ebc4fa34ff3>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the pre-trained BERT model and tokenizer for summarization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bert-base-uncased\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSummarization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BertForSummarization' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5"
      ],
      "metadata": {
        "id": "HqRSVhh0UY2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the T5 model and tokenizer\n",
        "model_name = \"t5-small\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define a function for text summarization\n",
        "def generate_summary(text):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "train['pred_title'] = \"\"  # Create a new column for predicted summaries\n",
        "test['pred_title'] = \"\"   # Create a new column for predicted summaries\n",
        "\n",
        "for index, row in train.iterrows():\n",
        "    content = row['content']\n",
        "    summary = generate_summary(content)\n",
        "    train.at[index, 'pred_title'] = summary  # Save the predicted summary in the new column\n",
        "\n",
        "for index, row in test.iterrows():\n",
        "    content = row['content']\n",
        "    summary = generate_summary(content)\n",
        "    test.at[index, 'pred_title'] = summary  # Save the predicted summary in the new column\n",
        "\n",
        "# Calculate BLEU scores using the generated summaries and 'title' as reference\n",
        "train_bleu_scores = []\n",
        "test_bleu_scores = []\n",
        "\n",
        "for index, row in train.iterrows():\n",
        "    generated_summary = row['pred_title']\n",
        "    reference_summary = row['title']\n",
        "    bleu_score = sentence_bleu([reference_summary.split()], generated_summary.split())\n",
        "    train_bleu_scores.append(bleu_score)\n",
        "\n",
        "for index, row in test.iterrows():\n",
        "    generated_summary = row['pred_title']\n",
        "    reference_summary = row['title']\n",
        "    bleu_score = sentence_bleu([reference_summary.split()], generated_summary.split())\n",
        "    test_bleu_scores.append(bleu_score)\n",
        "\n",
        "# Calculate the mean BLEU score\n",
        "mean_train_bleu = sum(train_bleu_scores) / len(train_bleu_scores)\n",
        "mean_test_bleu = sum(test_bleu_scores) / len(test_bleu_scores)\n",
        "\n",
        "print(\"Mean BLEU Score (Training):\", mean_train_bleu)\n",
        "print(\"Mean BLEU Score (Testing):\", mean_test_bleu)\n",
        "\n",
        "# Save the results back to your DataFrames\n",
        "train.to_csv('train_with_summaries.csv', index=False)\n",
        "test.to_csv('test_with_summaries.csv', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84oA0CqzFmkz",
        "outputId": "65ee6963-314a-46b4-dea4-67748ba30d4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Seq2seq"
      ],
      "metadata": {
        "id": "lW_hgq2Ti_Vf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(x_tr))"
      ],
      "metadata": {
        "id": "aQM7LQzUjDmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tot_cnt gives the size of vocabulary (which means every unique words in the text)\n",
        "\n",
        "cnt gives the no. of rare words whose count falls below threshold\n",
        "\n",
        "tot_cnt - cnt gives the top most common words\n"
      ],
      "metadata": {
        "id": "Wqqpv7HDzta6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thresh=4\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "\n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_8R3w82ztKC",
        "outputId": "8dc4f436-a362-4d3a-e6c2-3ffcdcb4d1ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% of rare words in vocabulary: 87.18411552346569\n",
            "Total Coverage of rare words: 44.46672012830794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n",
        "x_tokenizer.fit_on_texts(list(x_tr))\n",
        "\n",
        "#convert text sequences into integer sequences (i.e one-hot encodeing all the words)\n",
        "x_tr_seq    =   x_tokenizer.texts_to_sequences(x_tr)\n",
        "x_val_seq   =   x_tokenizer.texts_to_sequences(x_val)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "x_tr    =   pad_sequences(x_tr_seq,  maxlen=200, padding='post')\n",
        "x_val   =   pad_sequences(x_val_seq, maxlen=200, padding='post')\n",
        "\n",
        "#size of vocabulary ( +1 for padding token)\n",
        "x_voc   =  x_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in X = {}\".format(x_voc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9IWJWEfz302",
        "outputId": "41166dbc-0813-4348-cbe3-cb618385d095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary in X = 356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(y_tr))"
      ],
      "metadata": {
        "id": "5X3qk5IRz6Zl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thresh=6\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "\n",
        "print(\"% of rare words in vocabulary:\",(cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\",(freq/tot_freq)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E52-fL-c0mq_",
        "outputId": "f6e8c013-5aba-47f3-cb7f-91d6bbdbadcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% of rare words in vocabulary: 78.14271452019561\n",
            "Total Coverage of rare words: 9.931160887982037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt)\n",
        "y_tokenizer.fit_on_texts(list(y_tr))\n",
        "\n",
        "#convert text sequences into integer sequences (i.e one hot encode the text in Y)\n",
        "y_tr_seq    =   y_tokenizer.texts_to_sequences(y_tr)\n",
        "y_val_seq   =   y_tokenizer.texts_to_sequences(y_val)\n",
        "\n",
        "#padding zero upto maximum length\n",
        "y_tr    =   pad_sequences(y_tr_seq, maxlen=35, padding='post')\n",
        "y_val   =   pad_sequences(y_val_seq, maxlen=35, padding='post')\n",
        "\n",
        "#size of vocabulary\n",
        "y_voc  =   y_tokenizer.num_words +1\n",
        "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYY4vfBA0rTL",
        "outputId": "a520336d-6c64-4342-9ff5-1f99b8a8e1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary in Y = 12918\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ind=[]\n",
        "for i in range(len(y_tr)):\n",
        "    cnt=0\n",
        "    for j in y_tr[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_tr=np.delete(y_tr,ind, axis=0)\n",
        "x_tr=np.delete(x_tr,ind, axis=0)"
      ],
      "metadata": {
        "id": "6XY-OLvb1co2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ind=[]\n",
        "for i in range(len(y_val)):\n",
        "    cnt=0\n",
        "    for j in y_val[i]:\n",
        "        if j!=0:\n",
        "            cnt=cnt+1\n",
        "    if(cnt==2):\n",
        "        ind.append(i)\n",
        "y_val=np.delete(y_val,ind, axis=0)\n",
        "x_val=np.delete(x_val,ind, axis=0)"
      ],
      "metadata": {
        "id": "YO5O0gGF1oAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Size of vocabulary from the w2v model = {}\".format(x_voc))\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "latent_dim = 300\n",
        "embedding_dim=200\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(200,))\n",
        "\n",
        "#embedding layer\n",
        "enc_emb =  Embedding(x_voc, embedding_dim,trainable=True)(encoder_inputs)\n",
        "\n",
        "#encoder lstm 1\n",
        "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n",
        "\n",
        "#encoder lstm 2\n",
        "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n",
        "\n",
        "#encoder lstm 3\n",
        "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\n",
        "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "\n",
        "#embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim,trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\n",
        "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n",
        "\n",
        "#dense layer\n",
        "decoder_dense =  TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpQEvkCc1gnR",
        "outputId": "ebb07a45-972a-4a6e-effc-9df7b7a66ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of vocabulary from the w2v model = 356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]                0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 200, 200)             71200     ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 [(None, 200, 300),           601200    ['embedding[0][0]']           \n",
            "                              (None, 300),                                                        \n",
            "                              (None, 300)]                                                        \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               [(None, 200, 300),           721200    ['lstm[0][0]']                \n",
            "                              (None, 300),                                                        \n",
            "                              (None, 300)]                                                        \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, None, 200)            2583600   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)               [(None, 200, 300),           721200    ['lstm_1[0][0]']              \n",
            "                              (None, 300),                                                        \n",
            "                              (None, 300)]                                                        \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)               [(None, None, 300),          601200    ['embedding_1[0][0]',         \n",
            "                              (None, 300),                           'lstm_2[0][1]',              \n",
            "                              (None, 300)]                           'lstm_2[0][2]']              \n",
            "                                                                                                  \n",
            " time_distributed (TimeDist  (None, None, 12918)          3888318   ['lstm_3[0][0]']              \n",
            " ributed)                                                                                         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 9187918 (35.05 MB)\n",
            "Trainable params: 9187918 (35.05 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=2)"
      ],
      "metadata": {
        "id": "sEzGKg6t2Nu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start fitting the model with the data\n",
        "\n",
        "history=model.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,\n",
        "                  epochs=20,\n",
        "                  callbacks=[es],\n",
        "                  batch_size=64,\n",
        "                  validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vSJdXij2SLs",
        "outputId": "0e68273a-c068-4517-bb31-c4b3b4c60237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "11/11 [==============================] - 90s 5s/step - loss: 9.4621 - val_loss: 9.4564\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - 38s 4s/step - loss: 9.4515 - val_loss: 9.4419\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - 38s 4s/step - loss: 9.3145 - val_loss: 8.7713\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - 41s 4s/step - loss: 8.4135 - val_loss: 8.2444\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - 37s 3s/step - loss: 8.0979 - val_loss: 8.1490\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - 39s 4s/step - loss: 8.0108 - val_loss: 8.1447\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - 41s 3s/step - loss: 7.9629 - val_loss: 8.1459\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - 39s 4s/step - loss: 7.9349 - val_loss: 8.1533\n",
            "Epoch 8: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "DcXXq07o773F",
        "outputId": "50ec3884-9d96-46b2-db74-f0c0570149ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABVqklEQVR4nO3dd3hUZf738ffMJJNGEgiQkECAUEMTUQQprjRBZFnWtSK7NPEnio+iqyvs2hAVdZW1LnZEii4WLIsSIwgK0hGW3jshoabXmXn+mCQkSCCTTObMZD6v65prTmZO+U4s88m5z/neJofD4UBERETEIGajCxARERH/pjAiIiIihlIYEREREUMpjIiIiIihFEZERETEUAojIiIiYiiFERERETGUwoiIiIgYKsDoAirDbrdz7NgxwsPDMZlMRpcjIiIileBwOMjMzCQuLg6zueLzHz4RRo4dO0Z8fLzRZYiIiEgVHD58mCZNmlT4vk+EkfDwcMD5YSIiIgyuRkRERCojIyOD+Pj40u/xivhEGCkZmomIiFAYERER8TGXusRCF7CKiIiIoRRGRERExFAKIyIiImIon7hmREREpCY4HA6Kioqw2WxGl+KTLBYLAQEB1W67oTAiIiJ+qaCggJSUFHJycowuxaeFhoYSGxuL1Wqt8j4URkRExO/Y7Xb279+PxWIhLi4Oq9WqppoucjgcFBQUcOLECfbv30/r1q0v2tjsYhRGRETE7xQUFGC324mPjyc0NNTocnxWSEgIgYGBHDx4kIKCAoKDg6u0H13AKiIifquqf8nLOe74HeqfgoiIiBhKYUREREQMpTAiIiLip5o3b84rr7xidBm6gFVERMSX9OnTh8svv9wtIWLt2rWEhYVVv6hq8usw8tn6I2w5mk6I1UJwgIXgQHPpclCgmeBACyGBFoIDi98rXg4qsxxo0cklERHxHg6HA5vNRkDApb/iGzZs6IGKLs2vw8jBTUtJ2bOHLILJdoSQTTDZjmCyCCaHYIoq8euxmE0EBzhDTNB5gaYkxARfINAEB1oICii/bojVXByEfrtuSPH6ZrPugxcRqQkOh4PcQmM6sYYEWirV52T06NEsW7aMZcuW8eqrrwIwc+ZMxowZw7fffstjjz3G5s2b+f7774mPj+ehhx5i1apVZGdn065dO6ZNm8aAAQNK99e8eXMmTpzIxIkTAefsuu+++y4LFy4kKSmJxo0b8/LLL/OHP/yhRj53Cb8OI7c7vqOx9b8Vvl9AILmmELIJIYdgshzBZDqCybQHFYeWEHIIItsWQlZumUBTHGpOEEw2IWQ5nM+Fbvh1WwPMpeEnOLA4yFgtBAeYLxh4ygWiMtsFBVgIsTpDTmJsOBHBgdWuTUTEl+UW2mj/RJIhx9729CBCrZf+jnj11VfZtWsXHTt25OmnnwZg69atAEyaNImXXnqJFi1aUK9ePQ4fPswNN9zAs88+S1BQEB999BFDhw5l586dNG3atMJjTJkyhRdffJF//vOfvP7664wYMYKDBw8SFRXlng97AX4dRhq36AiONCjIcj7yi59tBQBYKcTqKCSSjHMbmQBL1Y5nMwVQYA4lzxJKvimE3OJHNiHkFJ+RyXQEkWEPIsMWTLo9iLNFzp9LAk22LYjsohDO5gWTT2BxQdUTZrVwR/em3Nm7BY0iq9awRkREal5kZCRWq5XQ0FAaNWoEwI4dOwB4+umnue6660rXjYqKonPnzqU/T506lQULFvD1119z3333VXiM0aNHM3z4cACee+45XnvtNdasWcP1119fEx8J8PMwQt/Jzsf5igrOCyjZUJBZZjkL8jOL18kufj2zzPJ54aYoDwCLo4gQWwYhtozfHrMiF/kn5DAFYAsMpSggjEJLKAXFj3xzKHmmYHJNocVhp3j4qcyZnXRbEOm2YFLyA9iQEcG7P+/nw18O8KcuTfi/a1vQsmEdF3+ZIiK+LSTQwranBxl27Orq2rVruZ+zsrJ46qmnWLhwISkpKRQVFZGbm8uhQ4cuup/LLrusdDksLIyIiAjS0tKqXd/F+HcYqUiAFQKiINRNp6RsRZcIN5llQkz2pYNOoXNSJ5OjiICCDAIKMqjO+YxTCddyD5NZs/80/1l3mPnrDzOofSPu6dOSzvF13fIrEBHxdiaTqVJDJd7q/LtiHn74YZKTk3nppZdo1aoVISEh3HzzzRQUFFx0P4GB5YftTSYTdrvd7fWW5bu/dV9iCYCQus6HO9htZc7QnH8mxsWzOFmp1E9Zxvz/m8L6wh7MWLqPH7ansmjrcRZtPU7PlvW5p09LerdqoEmkRES8gNVqxWa79IW2K1asYPTo0dx4442A80zJgQMHari6qlEY8UVmCwRHOB/V9fldsHk+rHqLK//0Nu+NimJXaiZvL9vHVxuP8sveU/yy9xQd4iK4p09LBneMxaI7ekREDNO8eXNWr17NgQMHqFOnToVnLVq3bs0XX3zB0KFDMZlMPP744zV+hqOq1CTD3119j/N5y+eQeRyANjHhvHxrZ5b9rS9jejUnJNDC1mMZ3DfvV/q/vJS5qw+SZ9DtbyIi/u7hhx/GYrHQvn17GjZsWOE1INOnT6devXr07NmToUOHMmjQIK644goPV1s5JofD4TC6iEvJyMggMjKS9PR0IiLccDZAynt/EBxeBdc+Cn3//pu3z2QXMGvlAT785QBncwoBaFAniDt7JzDi6qa6LVhEfE5eXh779+8nISGhytPei9PFfpeV/f7WmRGBq8c7n9e+D4V5v3m7XpiViQPa8Mukfjzx+/bERQZzMiufFxbtoNe0JbywaAdpmb/dTkREpDJcDiOZmZlMnDiRZs2aERISQs+ePVm7dm2ltl2xYgUBAQFcfvnlrh5WalLiUIhoAjknYctnFa4Wag1gbO8Elj7Sl5du6Uzr6Dpk5hcxY+leer/wI39fsJkDJ7M9WLiIiNQGLoeRcePGkZyczOzZs9m8eTMDBw5kwIABHD169KLbnT17lpEjR9K/f/8qFys1xBIA3f/PubxqBlxi5M4aYObmK5uQNPF3vDuyK1c0rUtBkZ15qw/R7+Wl3DdvA1uOpnugcBERqQ1cumYkNzeX8PBwvvrqK4YMGVL6+pVXXsngwYN55plnKtz29ttvp3Xr1lgsFr788ks2btxY6SJ1zYgH5J6B6e2dPUxG/RcSrqn0pg6Hg7UHzjBj6R5+3Hmi9PVrWjfgnj4t6dGivm4LFhGvomtG3Mfj14wUFRVhs9l+c7CQkBCWL19e4XYzZ85k3759PPnkk5U6Tn5+PhkZGeUeUsNC6sHldziXV81waVOTyUS3hChmjunGt/dfw7DL4zCb4OfdJ7nj3dX88d+/sGhLCna7118rLSIiBnApjISHh9OjRw+mTp3KsWPHsNlszJkzh5UrV5KSknLBbXbv3s2kSZOYM2dOpaYzBpg2bRqRkZGlj/j4eFfKlKrqXnwh685v4fS+Ku2ifVwEr97ehWWP9GVkj2YEBZjZdPgs4+dsYMC/ljF/7WEKirzzPncRETGGy9eMzJ49G4fDQePGjQkKCuK1115j+PDhmM2/3ZXNZuOOO+5gypQptGnTptLHmDx5Munp6aWPw4cPu1qmVEWD1tDqOsABq9+p1q7io0J5elhHVkzqx319WxERHMC+E9n87fP/8bsXf+S9n/eRlV/knrpFRMSnVbnPSHZ2NhkZGcTGxnLbbbeRlZXFwoULy61z9uxZ6tWrh8VybgIgu92Ow+HAYrHw/fff069fv0seS9eMeNCexTDnT2ANh4e2uafLK5CVX8THqw/x3vJ9pGbkAxARHMCons0Z3bM59esEueU4IiKVoWtG3MfQPiNhYWHExsZy5swZkpKSGDZs2G/WiYiIYPPmzWzcuLH0MX78eNq2bcvGjRvp3r17VQ8vNaVlP2jQ1jl/zca5btttnaAA7vpdC376W19euKkTLRqEkZFXxOtL9tDrhSU88dUWDp/OcdvxRETEd7gcRpKSkli0aBH79+8nOTmZvn37kpiYyJgxYwDnEMvIkSOdOzeb6dixY7lHdHQ0wcHBdOzY8TczDIoXMJnONUFb/ZZzUj43CgqwcNtVTUl+6Fre+vMVXNYkkrxCOx+tPEifl5Yy8ZNf2XFcFyyLiFSkT58+TJw40W37Gz16NH/84x/dtr+qcDmMpKenM2HCBBITExk5ciS9e/cmKSmpdMrhlJSUCvvki4+47HYIrgtnDsCuRTVyCIvZxPUdY/lqQi/mjevONa0bYLM7+HLjMa5/5WfGfriWtQdO18ixRUTEu2huGrmwH56C5f+C5tfA6P965JBbjqYzY9levtucQsldwFc2q8c917akX2I0Zs0WLCJu4qvXjIwePZpZs2aVe23//v1kZWXxyCOP8PPPPxMWFsbAgQP517/+RYMGDQD47LPPmDJlCnv27CE0NJQuXbrw1Vdf8c9//pMpU6aU29+PP/5Inz59Kl2TO64ZURiRC0s/Cq90AocNxi+HRp08duj9J7N556d9fL7+CAU2523AraPrMP7alvzh8jgCLZpSSUSq54JfoA6Hs/GjEQJDncPkl5Cens7gwYPp2LEjTz/9tHPTwEDatWvHuHHjGDlyJLm5uTz66KMUFRWxZMkSUlJSaNq0KS+++CI33ngjmZmZ/Pzzz6WXVNx5551kZGQwc+ZMAKKiorBarZUu3R1hpHKNP8T/RDaG9sNg6xew6i3445seO3RCgzCm/akTDw5ozQcrDjBn1UF2p2Xx1083MT15F+OuSeC2q+IJtepfXxFxo8IceC7OmGP//RhYL30dZWRkJFarldDQUBo1agTAM888Q5cuXXjuuedK1/vggw+Ij49n165dZGVlUVRUxJ/+9CeaNWsGQKdO5/7ADAkJIT8/v3R/RtCfmFKxq+91Pm+eD1knLr5uDYiOCGbS4ERWTOrH365vS4M6QRw9m8uUb7bR6/klvPrDbs5kF3i8LhERb7Jp0yZ+/PFH6tSpU/pITEwEYO/evXTu3Jn+/fvTqVMnbrnlFt59913OnDljcNXl6U9LqVj8VdC4KxxdB+s+gD6PGlJGZEgg9/ZpxdheCXy+4QhvL9vHodM5/OuHXby1bC/DuzVl3DUJxNUNMaQ+EaklAkOdZyiMOnYVZWVlMXToUF544YXfvBcbG4vFYiE5OZlffvmF77//ntdff51//OMfrF69moSEhOpU7TY6MyIXd/U9zud170NRvqGlBAdaGNG9GUv+ei2vD+9Ch7gIcgttfLBiP7978Uf+On8Tu1MzDa1RRHyYyeQcKjHi4cJkolarFZvtXNuFK664gq1bt9K8eXNatWpV7lHSQsNkMtGrVy+mTJnCr7/+itVqZcGCBRfcnxEURuTi2g+D8DjISoWtC4yuBoAAi5mhneP47//rzUdju9GjRX2K7A4+33CE6/71E3d9tI4Nh7zrFKSIiLs0b96c1atXc+DAAU6ePMmECRM4ffo0w4cPZ+3atezdu5ekpCTGjBmDzWZj9erVPPfcc6xbt45Dhw7xxRdfcOLECdq1a1e6v//973/s3LmTkydPUlhY6PHPpDAiF2cJhG7jnMsr33Rebe4lTCYTv2vTkI//72q+nNCL6zs0wmSC5G2p/Onfv3Db2yv5cWcaPnDDmIhIpT388MNYLBbat29Pw4YNKSgoYMWKFdhsNgYOHEinTp2YOHEidevWxWw2ExERwU8//cQNN9xAmzZteOyxx3j55ZcZPHgwAHfddRdt27ala9euNGzYkBUrVnj8M+nWXrm0nNMwvR0U5cGY76BZT6MrqtCetCze+WkvC349SqHN+a92u9gIxl/bgiGdYgnQbcEigu/2GfFGhs5NI34kNAo63+5cXvVvY2u5hFbRdXjx5s789Le+3HVNAqFWC9tTMnjgk430fXkps1ceIK/Q2LFREREpT2FEKqd78YWsOxbCmYPG1lIJsZEh/GNIe36Z1I+/XteGqDArh0/n8vhXW+n1/BLe/HEP6bmeHxcVEZHfUhiRyolOhBZ9wWGHNe8YXU2l1Q218v/6t2bFo/2Y8ocONK4bwqnsAv6ZtJNezy/huW+3k5qRZ3SZIiJ+TWFEKq+kCdqG2ZDvW7fQhlgtjOrZnKWP9OGV2y6nbUw4WflFvPPTPgZMX8ahUwa1gBYREYURcUGrAVC/FeSnw8aPja6mSgItZv7YpTGLJl7DzNFX0aJhGJl5RXy+4YjRpYmI+C2FEak8sxm6j3cur34L7HZj66kGk8lE38Ro7u3TCoBFW44bXJGIGMEHbij1eu74HSqMiGs6D4egSDi9F/YkG11NtV3XLoYAs4mdqZnsO5FldDki4iGBgYEA5ORoiLa6Sn6HJb/TqtDcNOKaoDpw5Uj45XXnbb5tBhldUbVEhgbSo2V9ft59ku+2HGdC31ZGlyQiHmCxWKhbty5paWkAhIaGYnKhJbs4z4jk5OSQlpZG3bp1sVgsVd6Xwoi4rtv/Obux7lsKqdsgpr3RFVXL4I6x/Lz7JIsURkT8SqNGjQBKA4lUTd26dUt/l1WlMCKuq9sU2g2FbV/B6hnwh9eNrqhaBnaI4bEvN7P5aDqHT+cQH1X12TNFxHeYTCZiY2OJjo42ZD6W2iAwMLBaZ0RKKIxI1Vx9rzOM/G8+9H8KwuobXVGVNagTRLeEKFbtO82iLce563ctjC5JRDzIYrG45QtVqk4XsErVxHeH2Mud89Wsn2l0NdU2uGMsAN9tSTG4EhER/6MwIlVjMp1rgrb2PSgqMLaeahrUwTneueHQWY6nqyOriIgnKYxI1XW4EerEQGaKc8jGhzWKDOaKpnUBSNqqniMiIp6kMCJVF2CFq+5yLq/6N/h486CSoZpvN2uoRkTEkxRGpHq6jgFLEBzbAEfWGl1NtVzf0TlUs/bAaU5m5RtcjYiI/1AYkeoJawCX3eJcXvVvY2uppvioUDo1jsTugO+3phpdjoiI31AYkerrfo/zedvXcPawsbVUU8nZEd1VIyLiOQojUn2NOkLC78Bhg7XvGl1NtQwuDiMr954iPUdNkEREPEFhRNyj5OzI+llQkG1sLdXQomEd2saEU2R3kLxdQzUiIp6gMCLu0WYQ1EuAvLOw6ROjq6mWwZ2Kh2p0V42IiEcojIh7mC3QfbxzefVbYLcbW081lNzi+/Puk2TmaahGRKSmKYyI+3QZAUERcHIX7F1idDVV1iamDi0ahFFgs7Nkh2bzFBGpaQoj4j5B4dDlL87l1TOMraUaTCZT6V01i7aoG6uISE1TGBH36nYXYII9P8CJnUZXU2UlQzVLd54gp6DI4GpERGo3l8NIZmYmEydOpFmzZoSEhNCzZ0/Wrq248+YXX3zBddddR8OGDYmIiKBHjx4kJSVVq2jxYlEJkDjEubz6LWNrqYaOjSNoUi+E3EIby3aeMLocEZFazeUwMm7cOJKTk5k9ezabN29m4MCBDBgwgKNHj15w/Z9++onrrruOb7/9lvXr19O3b1+GDh3Kr7/+Wu3ixUtdXXyb78aPIee0sbVUkclkKu058p2GakREapTJ4aj87Ga5ubmEh4fz1VdfMWTIkNLXr7zySgYPHswzzzxTqf106NCB2267jSeeeKJS62dkZBAZGUl6ejoRERGVLVeM4nDA29fA8c0wYAr0nmh0RVWy/uAZbprxC3WCAlj/+ACCAixGlyQi4lMq+/3t0pmRoqIibDYbwcHB5V4PCQlh+fLlldqH3W4nMzOTqKioCtfJz88nIyOj3EN8iMl0rgnamnfA5pu3x3aJr0tMRBBZ+UUs333S6HJERGotl8JIeHg4PXr0YOrUqRw7dgybzcacOXNYuXIlKSmVaxD10ksvkZWVxa233lrhOtOmTSMyMrL0ER8f70qZ4g063gRhDSHjKGz/xuhqqsRsNnF9Bw3ViIjUNJevGZk9ezYOh4PGjRsTFBTEa6+9xvDhwzGbL72refPmMWXKFObPn090dHSF602ePJn09PTSx+HDvj35ml8KDIaudzqXV/nubb7XF99Vk7wtlUKb7zZyExHxZi6HkZYtW7Js2TKysrI4fPgwa9asobCwkBYtWlx0u08++YRx48Yxf/58BgwYcNF1g4KCiIiIKPcQH9R1LFiscGQNHFlvdDVV0i0hivphVtJzC1m595TR5YiI1EpV7jMSFhZGbGwsZ86cISkpiWHDhlW47scff8yYMWP4+OOPy134KrVceIxzuAZ8tgmaxWxioIZqRERqlMthJCkpiUWLFrF//36Sk5Pp27cviYmJjBkzBnAOsYwcObJ0/Xnz5jFy5EhefvllunfvzvHjxzl+/Djp6enu+xTivUrmq9m6ADKOGVtLFZXc4pu87Tg2e6VvPhMRkUpyOYykp6czYcIEEhMTGTlyJL179yYpKYnAwEAAUlJSOHToUOn677zzDkVFRUyYMIHY2NjSxwMPPOC+TyHeK+5yaNYL7EWw9j2jq6mSHi3rExkSyMmsAtYe8M2+KSIi3sylPiNGUZ8RH7f9G/jPnyEkCh7aBoEhRlfksr/O38TnG44wumdznvpDB6PLERHxCTXSZ0SkStreAHWbQu5p+N98o6upkhs6lVw3koJdQzUiIm6lMCI1z2yBbnc7l1fNcHZo9TG9WzegTlAAqRn5/Hr4rNHliIjUKgoj4hlX/AWsdeDEdti31OhqXBYUYKFforM3zqItlWvwJyIilaMwIp4RHAmXj3Au+2gTtLIT5/nApVYiIj5DYUQ8p/vdgAl2J8HJPUZX47Jr2zYkONDMkTO5bD2m+ZJERNxFYUQ8p35LaDPIubzmbWNrqYJQawB92zqHar7drKEaERF3URgRz7q6eDbfX+dC7llDS6mK64uHahZpqEZExG0URsSzEq6F6PZQmA2/zja6Gpf1S4zGajGz72Q2u1KzjC5HRKRWUBgRzzKZzp0dWf0O2IqMrcdF4cGBXNO6AeDsOSIiItWnMCKe1+kWZzfW9EOw81ujq3FZ2aEaERGpPoUR8bzAEOg61rnsg7f5Xtc+hgCziR3HM9l3QkM1IiLVpTAixrhqHJgD4NAvcOxXo6txSd1QKz1a1gecPUdERKR6FEbEGBGx0OFPzuVVbxlbSxUM7hgLaKhGRMQdFEbEOCUXsm75HDJ960t9YIcYzCbYfDSdw6dzjC5HRMSnKYyIcRpfAfHdwV4I6z4wuhqXNKgTxFXNowBI2upbQUpExNsojIixSs6OrH0fCvOMrcVFZeeqERGRqlMYEWMlDoWIJpBzErZ8ZnQ1Lrm++LqR9QfPcDzdt4KUiIg3URgRY1kCoPv/OZdXzQAfarHeKDKYK5rWBTRUIyJSHQojYrwrRkJgKKRugQPLja7GJSV31agbq4hI1SmMiPFC6kHn4c5lH2uCVtKNdc3+05zKyje4GhER36QwIt6h+3jn885v4fQ+Y2txQXxUKB0bR2B3wPfbUo0uR0TEJymMiHdo2AZaXQc4nBPo+ZCSoZpvN2uoRkSkKhRGxHtcXXx25Nc5kJdhbC0uKLnFd+XeU6TnFBpcjYiI71EYEe/Rsj80aAsFmbBxrtHVVFqLhnVoGxNOkd1B8nYN1YiIuEphRLyHyXTu7Mjqt8BuM7YeF5RcyLpId9WIiLhMYUS8y2W3Q3BdOHMAdi0yuppKG9zJGUZ+2n2SrPwig6sREfEtCiPiXayh0HWMc9mHbvNtGxNOiwZhFBTZWbIjzehyRER8isKIeJ+rxoHJAgd+huObja6mUkwmU+lQzXe6q0ZExCUKI+J9IptA+2HO5VVvGVuLC0pu8V268wS5Bb5zvYuIiNEURsQ7XX2v83nzfMg6YWwtldSxcQRN6oWQW2hj2S4N1YiIVJbCiHin+KugcVewFcC6D4yuplJMJhPXdygeqtmiifNERCpLYUS819X3OJ/XvgdFvjHvS8ldNYu3p5FfpKEaEZHKUBgR79V+GITHQnYabF1gdDWV0iW+HjERQWTlF7F890mjyxER8Qkuh5HMzEwmTpxIs2bNCAkJoWfPnqxdu/ai2yxdupQrrriCoKAgWrVqxYcffljVesWfWAKh213O5ZVvgsNhbD2VYDZrqEZExFUuh5Fx48aRnJzM7Nmz2bx5MwMHDmTAgAEcPXr0guvv37+fIUOG0LdvXzZu3MjEiRMZN24cSUlJ1S5e/MCVYyAgGI7/Dw6tNLqaSrm++K6a5G2pFNrsBlcjIuL9XAojubm5fP7557z44ov87ne/o1WrVjz11FO0atWKGTMu3KDqrbfeIiEhgZdffpl27dpx3333cfPNN/Ovf/3LLR9AarnQKLjsNufyqn8bW0sldUuIon6YlfTcQlbtO2V0OSIiXs+lMFJUVITNZiM4OLjc6yEhISxfvvyC26xcuZIBAwaUe23QoEGsXFnxX7n5+flkZGSUe4gfK7mQdcdCOHPQ2FoqwWI2MbBDDKChGhGRynApjISHh9OjRw+mTp3KsWPHsNlszJkzh5UrV5KScuGuk8ePHycmJqbcazExMWRkZJCbm3vBbaZNm0ZkZGTpIz4+3pUypbaJbgct+oLDDmveMbqaSilpgPb91uPY7N5/rYuIiJFcvmZk9uzZOBwOGjduTFBQEK+99hrDhw/HbHbfjTmTJ08mPT299HH48GG37Vt8VEkTtA2zIT/T2FoqoUfL+kSGBHIyq4C1B04bXY6IiFdzOUG0bNmSZcuWkZWVxeHDh1mzZg2FhYW0aNHigus3atSI1NTUcq+lpqYSERFBSEjIBbcJCgoiIiKi3EP8XKsBUL8V5KfDxo+NruaSAi1mBrRznhFcpKEaEZGLqvLpjLCwMGJjYzlz5gxJSUkMGzbsguv16NGDxYsXl3stOTmZHj16VPXQ4o/MZug+3rm8+i2we/9dKoOLJ85btOU4dg3ViIhUyOUwkpSUxKJFi9i/fz/Jycn07duXxMRExoxxTvs+efJkRo4cWbr++PHj2bdvH3/729/YsWMH//73v5k/fz4PPvig+z6F+IfOwyEoEk7vhT3JRldzSb1bNyDMauF4Rh4bj5w1uhwREa/lchhJT09nwoQJJCYmMnLkSHr37k1SUhKBgYEApKSkcOjQodL1ExISWLhwIcnJyXTu3JmXX36Z9957j0GDBrnvU4h/CKoDVxYHXR+4zTc40EJ/DdWIiFySyeHw/raWGRkZREZGkp6erutH/N3ZQ/BqZ+edNfeshJj2Rld0Ud9tTuGeuRtoUi+En//WF5PJZHRJIiIeU9nvb81NI76lblNI/L1zefWFG+15k2vbNiQ40MyRM7lsPaZ+OSIiF6IwIr6n5Dbf/82HbO/ucBpqDaBPm2gAvtty4V48IiL+TmFEfE/TqyH2cijKg/Uzja7mkgZ3Ojdxng+MioqIeJzCiPgek+nc2ZG170FRgbH1XEK/xGisFjP7TmSzOy3L6HJERLyOwoj4pg43Qp0YyEyBbV8ZXc1FhQcHck3rBgB8u1lDNSIi51MYEd8UYIWrxjmXV/0bvHz44/oyDdBERKQ8hRHxXVeOAUsQHNsAR9YaXc1FXdc+hgCziR3HM9l/MtvockREvIrCiPiuOg3hslucy17eBK1uqJUeLesDuqtGROR8CiPi27rf43ze9jWc9e7ZnTVUIyJyYQoj4tsadYTm14DDBmvfNbqaixrYvhFmE/zvSDpHzuQYXY6IiNdQGBHfV3Kb7/pZUOC912M0DA/iquZRgM6OiIiUpTAivq/NIKiXAHlnYdMnRldzUYM7nmuAJiIiTgoj4vvMFug+3rm8+i2w242t5yKu7xgLwPqDZ0jNyDO4GhER76AwIrXD5XeANRxO7oK9S4yupkKNIoPp0rQuAElbdXZERAQURqS2CI6AK/7iXPby2XxvKD478t1mhREREVAYkdqk2/8BJtjzA5zYaXQ1FSq5xXf1/lOcyso3uBoREeMpjEjtEZUAiUOcy6vfMraWi4iPCqVj4wjsDvh+W6rR5YiIGE5hRGqXkgtZN34MOaeNreUiBpcM1eiuGhERhRGpZZr3hphOUJQLG2YZXU2FSoZqftlzkvScQoOrERExlsKI1C4mE1xd3CJ+zbtg884v+pYN69A2Jpwiu4MftmuoRkT8m8KI1D4db4KwhpBxFLZ/Y3Q1FbpeDdBERACFEamNAoOh653O5VXee5vv4E7OMPLT7hNk5RcZXI2IiHEURqR26joWzIFwZA0cWW90NRfUNiachAZhFBTZWbIjzehyREQMozAitVN4DHS62bnspU3QTCZT6VDNoi0pBlcjImIchRGpvUpu8926ADKOGVtLBUomzvtxxwlyC2wGVyMiYgyFEam94i6HZr3AXgRr3zO6mgvq1DiSxnVDyC20sWzXCaPLERExhMKI1G4lZ0fWzYSCHGNruQCTyVR6duQ7DdWIiJ9SGJHaLXEI1G0Kuadh83yjq7mgkrtqlmxPI79IQzUi4n8URqR2M1ug293O5VVvgcNhbD0X0CW+HjERQWTmF7Fiz0mjyxER8TiFEan9rvgLWOvAie2wb6nR1fyG2WxiUIfioZrNaoAmIv5HYURqv+BIuPwO57KXNkErmTgveXsqhTa7wdWIiHiWwoj4h5ILWXcnwck9xtZyAd0SoqgfZuVsTiGr9p0yuhwREY9SGBH/UL8ltLneubzmbWNruQCL2cTADjGA5qoREf/jUhix2Ww8/vjjJCQkEBISQsuWLZk6dSqOS1wUOHfuXDp37kxoaCixsbGMHTuWU6f01594WMlsvr/OhdyzhpZyIdcXD9V8v/U4Nrv3XWgrIlJTXAojL7zwAjNmzOCNN95g+/btvPDCC7z44ou8/vrrFW6zYsUKRo4cyZ133snWrVv59NNPWbNmDXfddVe1ixdxScK1EN0eCrPh19lGV/MbPVrUJyI4gJNZBaw7cNrockREPMalMPLLL78wbNgwhgwZQvPmzbn55psZOHAga9asqXCblStX0rx5c+6//34SEhLo3bs3d99990W3EakRJtO5a0dWvwM275op1xpg5rr2JQ3QNFQjIv7DpTDSs2dPFi9ezK5duwDYtGkTy5cvZ/DgwRVu06NHDw4fPsy3336Lw+EgNTWVzz77jBtuuKHCbfLz88nIyCj3EHGLy26FkChIPwQ7FxpdzW+UdGNN2nocu4ZqRMRPuBRGJk2axO23305iYiKBgYF06dKFiRMnMmLEiAq36dWrF3PnzuW2227DarXSqFEjIiMjefPNNyvcZtq0aURGRpY+4uPjXSlTpGKBIdB1rHN51VvG1nIBvVs3IMxqISU9j41HzhpdjoiIR7gURubPn8/cuXOZN28eGzZsYNasWbz00kvMmjWrwm22bdvGAw88wBNPPMH69etZtGgRBw4cYPz48RVuM3nyZNLT00sfhw8fdqVMkYu7ahyYA+DQL3DsV6OrKSc40EK/ds67ahZpqEZE/ITJcalbYcqIj49n0qRJTJgwofS1Z555hjlz5rBjx44LbvOXv/yFvLw8Pv3009LXli9fzjXXXMOxY8eIjY295HEzMjKIjIwkPT2diIiIypYrUrHPx8HmT+Gy2+FP3nWr77ebU7h37gbio0L46ZG+mEwmo0sSEamSyn5/u3RmJCcnB7O5/CYWiwW7veKOkRVtA1zylmCRGtO9+DbfLZ9DpnedgejTtiHBgWYOn85l6zFdLyUitZ9LYWTo0KE8++yzLFy4kAMHDrBgwQKmT5/OjTfeWLrO5MmTGTlyZLltvvjiC2bMmMG+fftYsWIF999/P926dSMuLs59n0TEFU2uhPjuYC+EdR8YXU05odYA+rSJBjRUIyL+waUw8vrrr3PzzTdz77330q5dOx5++GHuvvtupk6dWrpOSkoKhw4dKv159OjRTJ8+nTfeeIOOHTtyyy230LZtW7744gv3fQqRqihpgrb2fSjMM7aW8wzu5Lyr5tstKTqDKCK1nkvXjBhF14xIjbAVwaudIeMIDJoGPe41uqJSmXmFXDn1Bwpsdr5/8He0iQk3uiQREZfVyDUjIrWKJQCu/ZtzedkLkOM9XU/DgwPp3boBAN9t1lCNiNRuCiPi37r82dkiPu8s/Pyy0dWUc33Hkm6sKQZXIiJSsxRGxL+ZLXBd8TVPq9+G0/uNraeMge1jCDCb2HE8kwMns40uR0SkxiiMiLTqDy36Ou+sWTzF6GpK1Q210qNlfUBz1YhI7aYwImIywcBnABNsXQCHvWcSRw3ViIg/UBgRAWjUEboUz7GU9A/wkpvMBrZvhMkE/zuSzpEzOUaXIyJSIxRGREr0/QcEhsKRNbDtK6OrAaBheBBXNY8C1ABNRGovhRGREhFx0PP/OZd/eBKKCoytp9gNxUM1CiMiUlspjIiU1fN+qBMDZw7A2neNrgaA6zs6J5Ncf+gMqRne1SlWRMQdFEZEygqq4xyuAVj2olc0QmsUGUyXpnVxOCBpq86OiEjtozAicj4vbIQ2uOSuGnVjFZFaSGFE5Hxe2AhtcPFQzer9pziVlW9wNSIi7qUwInIhrQdAy37ORmg/PGV0NcRHhdKxcQR2ByRvSzW6HBERt1IYEanIdVMBE2z70isaoZWcHVE3VhGpbRRGRCriZY3QSrqxrthzkvScQkNrERFxJ4URkYvp+5jXNEJr2bAObWLqUGR38MN2DdWISO2hMCJyMRGxzt4jUNwIzdiLR6/XUI2I1EIKIyKX0vP/lWmE9p6hpdzQyTlU89PuE2TlFxlai4iIuyiMiFyKFzVCaxsTTkKDMAqK7Py4I82wOkRE3ElhRKQyvKQRmslkKr2QVXPViEhtoTAiUhlmCwws2whtn2GllHRjXbIjjdwCm2F1iIi4i8KISGW1KtsIbYphZXRqHEnjuiHkFtpYtuuEYXWIiLiLwoiIK7ygEVr5oZoUQ2oQEXEnhRERV3hJI7SSu2oWb08jv0hDNSLi2xRGRFxVrhHal4aU0CW+HjERQWTmF/HLnlOG1CAi4i4KIyKuKtcI7SlDGqGZzSYGdXCeHfl2s4ZqRMS3KYyIVIUXNEIruW4keXsqhTa7ITWIiLiDwohIVQTVgX6POZcNaoTWrXkUUWFWzuYUsnqfcY3YRESqS2FEpKouH3GuEdpPL3n88AEWM4M6xADwne6qEREfpjAiUlVlG6GteceQRmglE+clbU3FZjfmzh4RkepSGBGpDoMbofVoUZ+I4ABOZuWz/uAZjx9fRMQdFEZEqmvgM2AyG9IIzRpgZkB751CN7qoREV+lMCJSXTEdnNePACT93eON0AaXDtUcx66hGhHxQS6FEZvNxuOPP05CQgIhISG0bNmSqVOn4rjE/3zz8/P5xz/+QbNmzQgKCqJ58+Z88MEH1SpcxKv0/UdxI7S1Hm+Edk3rBoRZLaSk57HpyFmPHltExB0CXFn5hRdeYMaMGcyaNYsOHTqwbt06xowZQ2RkJPfff3+F2916662kpqby/vvv06pVK1JSUrDb1RdBapGSRmjLnnc2Qmt7AwQEeeTQwYEW+rWL4ZtNx1i05ThdmtbzyHFFRNzFpTDyyy+/MGzYMIYMGQJA8+bN+fjjj1mzpuJx8kWLFrFs2TL27dtHVFRU6XYitU6v+2H9h+caofWY4LFDD+7YiG82HeO7LceZNDgRk8nksWOLiFSXS8M0PXv2ZPHixezatQuATZs2sXz5cgYPHlzhNl9//TVdu3blxRdfpHHjxrRp04aHH36Y3NzcCrfJz88nIyOj3EPE61nDoN8/nMseboTWp21DggPNHDqdw9Zj+u9FRHyLS2Fk0qRJ3H777SQmJhIYGEiXLl2YOHEiI0aMqHCbffv2sXz5crZs2cKCBQt45ZVX+Oyzz7j33nsr3GbatGlERkaWPuLj410pU8Q4BjVCC7UGcG2bhgAs2nLcY8cVEXEHl8LI/PnzmTt3LvPmzWPDhg3MmjWLl156iVmzZlW4jd1ux2QyMXfuXLp168YNN9zA9OnTmTVrVoVnRyZPnkx6enrp4/Dhw659KhGjGNgI7YZOzrtq1I1VRHyNS2HkkUceKT070qlTJ/7yl7/w4IMPMm3atAq3iY2NpXHjxkRGRpa+1q5dOxwOB0eOHLngNkFBQURERJR7iPiMVgOgZX+PN0LrlxiN1WJm74lsdqdmeuy4IiLV5VIYycnJwWwuv4nFYrnonTG9evXi2LFjZGVllb62a9cuzGYzTZo0cbFcER8xcOq5RmiHVnvkkOHBgfRu3QCA7zRUIyI+xKUwMnToUJ599lkWLlzIgQMHWLBgAdOnT+fGG28sXWfy5MmMHDmy9Oc77riD+vXrM2bMGLZt28ZPP/3EI488wtixYwkJCXHfJxHxJmUboX3/D481Qru+YyNA3VhFxLe4FEZef/11br75Zu69917atWvHww8/zN13383UqVNL10lJSeHQoUOlP9epU4fk5GTOnj1L165dGTFiBEOHDuW1115z36cQ8UYGNEK7rl0MFrOJHcczOXAy2yPHFBGpLpPjUu1TvUBGRgaRkZGkp6fr+hHxLUufh6XToF5zmLDGI43Q/vzeapbvOcmj1ydyT5+WNX48EZGKVPb7W3PTiNSknv8P6jRyNkJb865HDjm4k3OoZpHuqhERH6EwIlKTyjZC+8kzjdAGtm+EyQSbjqRz9GzFzQVFRLyFwohITbt8BER3gLx0jzRCaxgexFXNnVMvqAGaiPgChRGRmmZAI7TBxXfVfKe7akTEByiMiHhCq/5lGqE9VeOHK7nFd/2hM6Rl5NX48UREqkNhRMRTShuhfVXjjdBiI0Po0rQuDgckbdVQjYh4N4UREU/xcCO00qEaXTciIl5OYUTEk/o95rFGaIM7OifOW73/NKey8mv0WCIi1aEwIuJJ4Y2g1wPO5eQnoajmQkJ8VCgd4iKw2R0kb0utseOIiFSXwoiIp5U0Qjt7sMYboWmoRkR8gcKIiKdZw5zDNVDjjdAGd3IO1fyy9yTpuYU1dhwRkepQGBExwuV3eKQRWsuGdWgTU4dCm4PF2zVUIyLeSWFExAjnN0I7tbfGDnV98YWsGqoREW+lMCJilLKN0BZPqbHDlFw3smzXCbLyi2rsOCIiVaUwImKkgc/UeCO0xEbhNK8fSkGRnR93pNXIMUREqkNhRMRIMe2hy5+dyzXUCM1kMpUO1WjiPBHxRgojIkbr+49zjdC2LqiRQ9zQyTlU8+PONPIKbTVyDBGRqlIYETFa2UZoPzxVI43QOjWOpHHdEHIKbCzbdcLt+xcRqQ6FERFvUMON0JxDNc6zIxqqERFvozAi4g080Ait5K6aH7alkl+koRoR8R4KIyLeolwjtH+6ffdXNK1HdHgQmflF/LLnlNv3LyJSVQojIt6iXCO0d93eCM1sPjdU892WFLfuW0SkOhRGRLxJq/7QakCNNUIrCSPJ21Ipstndvn8RkapQGBHxNtdNLdMIbZVbd92teRRRYVbO5BSyen/NTdAnIuIKhRERb1O2EVqSexuhBVjMDGwfA8C3mzVUIyLeQWFExBv1/QcEhsHRdW5vhFYyVJO0NRWb3f0dX0VEXKUwIuKNarARWs+WDYgIDuBkVj7rD55x235FRKpKYUTEW/W8r0wjtHfctltrgJkBxUM1uqtGRLyBwoiItyrXCO2fbm2ENrh44rykLcdx1MDkfCIirlAYEfFml98BMR3d3gjtmtYNCLNaOJaex6Yj6W7br4hIVSiMiHizGmqEFhxooW9iNADf6a4aETGYwoiIt2vZ71wjtB+ecttub+jkHKr5TkM1ImIwhRERX1DSCG37125rhNanbUOCA80cOp3DtpQMt+xTRKQqXAojNpuNxx9/nISEBEJCQmjZsiVTp06t9F9VK1asICAggMsvv7wqtYr4rxpohBZqDeDaNg0BWLDhaLX3JyJSVS6FkRdeeIEZM2bwxhtvsH37dl544QVefPFFXn/99Utue/bsWUaOHEn//v2rXKyIX6uBRmg3dmkMwHvL9/PFhiNu2aeIiKtcCiO//PILw4YNY8iQITRv3pybb76ZgQMHsmbNmktuO378eO644w569OhR5WJF/FoNNEIb1KERY3slAPDIZ//jh22p1d6niIirXAojPXv2ZPHixezatQuATZs2sXz5cgYPHnzR7WbOnMm+fft48sknK3Wc/Px8MjIyyj1EBGcjtPBYtzVCM5lMPDakHTdd0QSb3cG98zawat8pNxQqIlJ5LoWRSZMmcfvtt5OYmEhgYCBdunRh4sSJjBgxosJtdu/ezaRJk5gzZw4BAQGVOs60adOIjIwsfcTHx7tSpkjtVQON0MxmEy/c1IkB7WIoKLIzbtY6thxV7xER8RyXwsj8+fOZO3cu8+bNY8OGDcyaNYuXXnqJWbNmXXB9m83GHXfcwZQpU2jTpk2ljzN58mTS09NLH4cPH3alTJHarfNwtzdCC7CYeeOOLlzdIoqs/CJGfrCGvSey3LJvEZFLMTlcaDAQHx/PpEmTmDBhQulrzzzzDHPmzGHHjh2/Wf/s2bPUq1cPi8VS+prdbsfhcGCxWPj+++/p16/fJY+bkZFBZGQk6enpREREVLZckdpr7xKYfSOYA2HCaqjf0i27zcwr5I53V7P5aDpxkcF8dk9P4uqGuGXfIuJ/Kvv97dKZkZycHMzm8ptYLBbsdvsF14+IiGDz5s1s3Lix9DF+/Hjatm3Lxo0b6d69uyuHF5ESNdQILTw4kA/HXEWLhmEcS8/jz++v5lSW+2YMFhG5EJfCyNChQ3n22WdZuHAhBw4cYMGCBUyfPp0bb7yxdJ3JkyczcuRI587NZjp27FjuER0dTXBwMB07diQsLMy9n0bEn9RAIzSA+nWCmHNnd+Iig9l3IpvRM9eSmVfotv2LiJzPpTDy+uuvc/PNN3PvvffSrl07Hn74Ye6++26mTp1auk5KSgqHDh1ye6Eicp6Y9tDlL85lNzVCKxFXN4TZ47pTP8zK5qPp3PXROvIKbW7bv4hIWS5dM2IUXTMiUoHM4/DaFVCYDTd/AB1vcuvutxxN5/Z3VpGVX8SAdjG89ecrCLBoFgkRqZwauWZERLxMDTRCK6tj40jeG9UVa4CZH7an8ujnm7Hbvf7vFxHxMQojIr6utBHaIbc0Qjvf1S3q8+87rsBiNvH5hiM8s3C7ZvkVEbdSGBHxdTXQCO18A9rH8M+bLwPggxX7eWPJHrcfQ0T8l8KISG1QthHashdr5BB/uqIJTw5tD8DLybuYvfJAjRxHRPyPwohIbWC2wMDiu9rWvgun9tbIYcb0SuCB/q0BeOLrrXy18WiNHEdE/IvCiEht0bIftLoO7EVubYR2vokDWjOqRzMcDvjr/E38uCOtxo4lIv5BYUSkNrnu6RpphFaWyWTiyaEd+OPlcRTZHYyfs541+91/nYqI+A+FEZHapAYboZVlNpv45y2d6Z8YTX6RnTs/XMvWY5rpV0SqRmFEpLbp+w8IDIOj62DrFzV2mECLmTdHXEG3hCgy84sY9cEa9p/MrrHjiUjtpTAiUtuEx0Dvic7lGmiEVlZwoIX3RnWlQ1wEJ7MK+PN7qzmenldjxxOR2klhRKQ26jGhRhuhlRURHMissd1IaBDG0bO5/OX91ZzJLqjRY4pI7aIwIlIbeaARWlkN6gQx+85uNIoIZndaFqM/XEtWflGNHlNEag+FEZHaygON0MpqUi+UOeO6US80kE2Hz3L37HXkF2mmXxG5NIURkdrKbIGBzziXa7ARWlmtosP5cEw3wqwWVuw5xQMfb6TIZq/x44qIb1MYEanNWvb1SCO0sjrH1+XdkV2xWsws2nqcvy/YrIn1ROSiFEZEaruBU881Qju40iOH7NmqAa/f0QWzCeavO8Jz32qmXxGpmMKISG0X3Q6uGOlc/r7mGqGdb1CHRrxwk3Om33d/3s+/l9b8MJGI+CaFERF/0OfvxY3Q1tdoI7Tz3dI1nseGtAPgn0k7mbv6oMeOLSK+Q2FExB94sBHa+cZd04L7+rYC4LEvt/DNpmMeO7aI+AaFERF/UbYR2uq3PXrovw5sw5+vborDAQ/N38iyXSc8enwR8W4KIyL+whoG/R53Lv/0Uo03QivLZDLx9B86MrRzHIU2B+Nnr2f9Qc30KyJOCiMi/qTz7RDTCfI90witLLPZxMu3dObaNg3JLbQxZuZatqdkeLQGEfFOCiMi/sRscd7qCx5rhFaWNcDMW3++kq7N6pGRV8TID9Zw8JRm+hXxdwojIv6mXCO0Jz1++BCrhfdHX0Vio3BOZObz5/dXk5qhmX5F/JnCiIg/Km2E9o3HGqGVFRkSyEd3dqNZ/VAOn85l5PtrOJujmX5F/JXCiIg/MqgRWrkSwoOZc2d3YiKC2JmayZgP15JToJl+RfyRwoiIvyrbCG3x03D2sMdLiI8KZfad3akbGsivh85y9+z1mulXxA8pjIj4q/AY6P2gc3n5dHilI7zbD5a/Aqf3eayMNjHhzBx9FaFWCz/vPslD/9mEza55bET8icnhA7NXZWRkEBkZSXp6OhEREUaXI1J72O2wfiZs/gwOrQTK/O+gUSdoPwzaDYOGbWq8lOW7TzL2w7UU2OwM7xbPczd2wmQy1fhxRaTmVPb7W2FERJwyU2HHN7DtaziwHBxlhksaJhYHkz9ATAeooZDw3eYUJszbgN0B469tyaTBiTVyHBHxDIUREam67FOwc6EzmOxbCvbCc+9FtYT2f3CGk9jL3R5M/rP2EI9+vhmAyYMTufvalm7dv4h4jsKIiLhH7lnYtQi2fQV7FoOtzCR7dZs6z5a0HwaNu4LZPZehvb1sL9O+2wHA83/qxO3dmrplvyLiWQojIuJ++Zmw+3tnMNmdDIU5594Lj4N2Q53BpOnVzm6v1fD8dzt4a9lezCZ4844rGNwptprFi4inVfb726U/Y2w2G48//jgJCQmEhITQsmVLpk6dysXyzBdffMF1111Hw4YNiYiIoEePHiQlJblyWBHxFkHh0PEmuPUjeGQv3DobOt0C1nDIPAZr3oYPb4CX28I3E2Hvj2ArvORuL+TR69syvFs8dgc88MlGft6tmX5FaiuXzow899xzTJ8+nVmzZtGhQwfWrVvHmDFjePbZZ7n//vsvuM3EiROJi4ujb9++1K1bl5kzZ/LSSy+xevVqunTpUqnj6syIiJcrzHNeW7LtK+e1Jnnp594LqQeJQ5x35bToAwHWSu/WZndw/8e/snBzCqFWC3PHdadL03puL19EakaNDNP8/ve/JyYmhvfff7/0tZtuuomQkBDmzJlT6eI6dOjAbbfdxhNPPFGp9RVGRHxIUQEc+Ml58euO/0LOqXPvBUVC2+udQzkt+0FgyCV3l19kY9ysdfy8+ySRIYHMv7sHbRuF1+AHEBF3qZFhmp49e7J48WJ27doFwKZNm1i+fDmDBw+u9D7sdjuZmZlERUVVuE5+fj4ZGRnlHiLiIwKs0GoA/OE1+OsuGPUNXHUX1GkE+enwv//AJ3fAiy3h0zGwdQEUVDxzb1CAhbf/ciVdmtYlPbeQv7y/msOncypcX0R8j0tnRux2O3//+9958cUXsVgs2Gw2nn32WSZPnlzpA7744os8//zz7Nixg+jo6Auu89RTTzFlypTfvK4zIyI+zG6HI2ucQznbvoaMI+feCwiBVv2h/R+hzSAI/u1/52dzCrjt7VXsTM2kWf1QPh3fg+jwYM/VLyIuq5Fhmk8++YRHHnmEf/7zn3To0IGNGzcyceJEpk+fzqhRoy65/bx587jrrrv46quvGDBgQIXr5efnk59/7vbBjIwM4uPjFUZEaguHA45ugO1fOcPJmQPn3rNYoUVf51BO28EQeu4salpGHje99QuHT+eS2Cic/9zdg8iQQM/XLyKVUiNhJD4+nkmTJjFhwoTS15555hnmzJnDjh07LrrtJ598wtixY/n0008ZMmRIZQ8J6JoRkVrN4YDjm2H7185gcnLXuffMAZDwO2cvk8TfQ52GHDyVzc1vreREZj5XNqvH7Du7EWoNMK5+EalQjVwzkpOTg/m8pkYWiwW73X7R7T7++GPGjBnDxx9/7HIQEZFazmSC2Mug32Nw31q4d7VzRuGYjmAvgr1L4L8T4eU28OHvabZ3Hh/f3pSI4ADWHzzDPXM2UFB08f8HiYh3c+nPiaFDh/Lss8/StGlTOnTowK+//sr06dMZO3Zs6TqTJ0/m6NGjfPTRR4BzaGbUqFG8+uqrdO/enePHjwMQEhJCZGSkGz+KiNQK0YnOR59H4dTe4mtMvoKUjXDgZzjwM60w8Uv0lbxxvD1f7+rKXz8N5JXbLsdi1sR6Ir7IpWGazMxMHn/8cRYsWEBaWhpxcXEMHz6cJ554AqvV2Ttg9OjRHDhwgKVLlwLQp08fli1b9pt9jRo1ig8//LBSx9UwjYhw5iBs/8YZTI6sKffWRnsLUhsPZODN/4epvuayEfEWagcvIrVXxrHiYPI1joMrMFHmf2MxnZwXv7b/AzRsa1yNIqIwIiJ+IiuN1d99RP7/vqSneSsBpjLXjzRMPDeRX0wHt88wLCIXpzAiIn7lzR/38G7SOq6zrOfBxtuJO7kK7GXmxYlq4Qwl7f4AcV0UTEQ8QGFERPyKw+Fg2nc7eOenfZhN8O6trelv/tV5y/CeH6Ao79zKkU2dwzjt/gBNrgKzSzcWinie3QZF+WArcE4+aSteLioofq3Mo/S1fOe6v9mu7GtlHl3HQuMr3Vq2woiI+B2Hw8Gjn/+P+euOYLWY+XDMVfRs1QDys2D3986LX3cnQ2GZ9vPhsRDfzdlszRwAZguYLMXLxT+by/xc+t55r7u03XnP5bYLcIajS24XoLM77mS3X+IL3JUv/sIK9lVwbp/nv3apfTs8cPv6Te9Dp5vdukuFERHxS0U2O/fN+5VFW48TZrUw766r6Rxf99wKhbmwZ7EzmOxaBPm+PPeVqeIQUxpkKggz54cqhwNwFD9z3s8XerZf5D0use2lnin+8q3itpVd//xj+BKLtfwjwHqB14LAEgiW4ueAoOL3LvSa1dnxOLqdW8tUGBERv5VfZGPsh2tZsecU9UID+XR8D1pFX2Cm36J82LcMzux3nga3FzkfDluZn897dpRZz247bzt7mffOe99Rwf4uuN/z9uNrX5S1gTng4l/sF/2yP3+bivZTNkRc6LWLhAgfOSumMCIifi0rv4gR761m0+GzNIoI5rN7etCkXqjRZVWN3V5xCLpYOLLbLhGeygQiTOe+4EymMj+7+Fydbcs944b9VPBZTObfvnZ++NB1RG6hMCIifu9MdgG3vr2S3WlZJDQIY/7dPWgYHmR0WSJ+o0bmphER8SX1wqzMvrM7jeuGsP9kNqM+WENGXuGlNxQRj1IYEZFarVFkMHPGdadBHSvbUjIY9+E68gptRpclImUojIhIrZfQIIxZY7sRHhTAmgOnuXfuBgptmulXxFsojIiIX+gQF8kHY64iONDMkh1pPPLpJux2r79kTsQvKIyIiN+4qnkUM0ZcSYDZxJcbjzHlm634wDX8IrWewoiI+JW+idG8fGtnTCaYtfIgr/yw2+iSRPyewoiI+J1hlzfm6T90AODVxbt5Y8luMnWXjYhh1GdERPzW64t383LyLgACLSa6JUTRPzGG/u2iaVY/zODqRHyfmp6JiFyCw+HggxUHmLvqIPtOZpd7r2XDMPq3i6FfYjRXNqtHoEUnkkVcpTAiIuKCfSeyWLIjjcXb01h74DRFZe60iQgO4Nq20fRPjObaNg2pF2Y1sFIR36EwIiJSRem5hfy8+wRLtqfx4840zuScu57EbIKuzaLo184ZTlpF18HkI5OWiXiawoiIiBvY7A42Hj7D4u1pLNmRxo7jmeXej48KoX+iczine4soggIsBlUq4n0URkREasCRMzmlwzkr956ioEwn11CrhWtaN6B/uxj6to3WpHzi9xRGRERqWHZ+ESv2nHSGkx1pnMjML/d+5/i69E+Mpl9iNB3iIjScI35HYURExIPsdgdbj2WweEcqi7ensfloern3G0UE0zfReZ1Jr1YNCLFqOEdqP4UREREDpWbk8WPxGZPlu0+SW2am4KAAM71aNaBf8VmTuLohBlYqUnMURkREvEReoY1V+06VXmty9GxuuffbxUY4h3PaRXN5k7qYzRrOkdpBYURExAs5HA52pWbxw/ZUluxIY8OhM5T9v3D9MGvpcE7v1g0IDw40rliRalIYERHxAaezC1i60zmc89POE2TmF5W+F2gx0T2hPv0So9WiXnySwoiIiI8ptNlZe+A0S7Y7w8n+ClrU9y9uUR+gFvXi5RRGRER83KVa1Pdp6zxjcm2bhtQNVYt68T4KIyIitYha1IsvUhgREamlSlrU/7A9jSXb09iZWr5FfdOo0NLrTLolqEW9GEdhRETETxw+ncOPOy/coj7MauGa1g3p1y5aLerF4xRGRET80MVa1JtM0LlJ3dKeJu1j1aJealaNhBGbzcZTTz3FnDlzOH78OHFxcYwePZrHHnvsov9CL126lIceeoitW7cSHx/PY489xujRo93+YURE5By73cGWY+mlMw5fqEV9yRmTTo0jiYkIUjgRt6rs93eAKzt94YUXmDFjBrNmzaJDhw6sW7eOMWPGEBkZyf3333/Bbfbv38+QIUMYP348c+fOZfHixYwbN47Y2FgGDRrk2qcSEZFKM5tNXNakLpc1qcuD17X5TYv64xl5zFt9iHmrDwEQHhRAq5g6tI6uQ+vo8NLluMgQdYWVGuXSmZHf//73xMTE8P7775e+dtNNNxESEsKcOXMuuM2jjz7KwoUL2bJlS+lrt99+O2fPnmXRokWVOq7OjIiIuFfZFvXL95zk4KkcbPYLfx2EBFpoFe0MJs6AEk7r6DrER4ViUUiRi6iRMyM9e/bknXfeYdeuXbRp04ZNmzaxfPlypk+fXuE2K1euZMCAAeVeGzRoEBMnTqxwm/z8fPLzz41zZmRkuFKmiIhcQnCghT5to+nTNhqA/CIbB07msDstk92pWew5kcWe1Cz2ncwit9DG5qPpvxnmsQaYadmw5ExKHVrH1KFVdDjN6ocSqIZs4gKXwsikSZPIyMggMTERi8WCzWbj2WefZcSIERVuc/z4cWJiYsq9FhMTQ0ZGBrm5uYSE/Ha2ymnTpjFlyhRXShMRkWoICrDQtlE4bRuFl3u90Gbn0OkcZ0BJy2R3Wha7U7PYeyKL/CI721My2J5S/g/GQIuJ5vXDSsNJSVBJaBCm24zlglwKI/Pnz2fu3LnMmzePDh06sHHjRiZOnEhcXByjRo1yW1GTJ0/moYceKv05IyOD+Ph4t+1fREQqJ9DiPPvRsmEdoFHp6za7gyNnnCFld1oWe9LOhZWcApsztKRlAcdLtzGboHn9MOeQT/FwT6to575DrAop/sylMPLII48wadIkbr/9dgA6derEwYMHmTZtWoVhpFGjRqSmppZ7LTU1lYiIiAueFQEICgoiKEj3wouIeCuL2USz+mE0qx/GgPbnzn7b7Q5SMvLYnZrJnuKzKLuLQ0pmXhH7Tmaz72Q23287971gMkGTeiGl16I4w4ozqNQJculrSnyUS/+Uc3JyMJvLjwNaLBbsdnsFW0CPHj349ttvy72WnJxMjx49XDm0iIj4ALPZROO6ITSuG1J6PQqAw+EgLTO/OKBklp452Z2ayZmcQg6fzuXw6VyW7Egrt7+4yGBaxYSXXpfSqvhOn8jQQE9/NKlBLoWRoUOH8uyzz9K0aVM6dOjAr7/+yvTp0xk7dmzpOpMnT+bo0aN89NFHAIwfP5433niDv/3tb4wdO5YlS5Ywf/58Fi5c6N5PIiIiXstkMhETEUxMRDC9WjUo996prPzScLKnTFA5kZnPsfQ8jqXn8dOuE+W2aRgedC6glAkr9evorLovcunW3szMTB5//HEWLFhAWloacXFxDB8+nCeeeAKr1Tlj5OjRozlw4ABLly4t3W7p0qU8+OCDbNu2jSZNmvD444+r6ZmIiFxUek4he05kll6XUhJWjqXnVbhNVJi19Dbk1mWGe6LD1dDNCGoHLyIitVJmXiF7T2SXXpeypzioHD6TQ0XfaOHBAaXN3Jx3+TiDSlxksEJKDVIYERERv5JbYGPviZJwUtwvJS2LA6eyqaCfG2FWZ0O3JlGhNKwTRHREUPFzcOnPUaFWdaCtIoURERERnA3d9p/MLg0nJWFl/8lsCm2X/gq0mE3UD7OeCyrhwTQMPxdcGoafe023KJdXIx1YRUREfE1QgIXERhEkNir/ZVhos3PwVA570jI5djaPE1n5pGXkFz/ncTIrn1PZBdjszjuB0srMgFyR8KAAGoYHlXuUhpfSn4Oop7Mt5SiMiIiIXwq0mGlVfLtwRQptdk5nF3AiM5+0zDznc2lgcT6XvJdXaCczv4jMfGc/lYuxmE00qGO9YFA5P8QEB9b+sy0KIyIiIhUItJhLb0mGyArXczgcZOUXkZZZEk6cz2VDTMmj5GxLakY+qRmVONsSHFAmqASfd23LueBSNyTQZ8+2KIyIiIhUk8lkIjw4kPDgwOLW+RUrtNk5lVVQLqSknRdcSoaFCorsZOYVObvXnrj42ZYAs4kG5weVOkE0LL4Yt+yZF28726IwIiIi4kGBFjONIoNpFBl80fUcDgeZ+UXO4aALnGEpPfuSlc/p7AKK7A6OZ+RxPKPiPiwlIkrPtgSXDgsNuzyOy5rUddOndI3CiIiIiBcymUxEBAcSERx40etaAAqK7JzKzi8TXEqCSt5vrnEpKLKTkVdERl4Re8ucbekcX1dhRERERKrGGmAmNjKE2MgLT0BbwuFwkJFb5Awp551ladco3EPV/pbCiIiIiJ8wmUxEhgYSGRpIq2jjwsf5zJdeRURERKTmKIyIiIiIoRRGRERExFAKIyIiImIohRERERExlMKIiIiIGEphRERERAylMCIiIiKGUhgRERERQymMiIiIiKEURkRERMRQCiMiIiJiKIURERERMZRPzNrrcDgAyMjIMLgSERERqayS7+2S7/GK+EQYyczMBCA+Pt7gSkRERMRVmZmZREZGVvi+yXGpuOIF7HY7x44dIzw8HJPJ5Lb9ZmRkEB8fz+HDh4mIiHDbfn2Jv/8O/P3zg34H+vz+/flBv4Oa/PwOh4PMzEzi4uIwmyu+MsQnzoyYzWaaNGlSY/uPiIjwy38By/L334G/f37Q70Cf378/P+h3UFOf/2JnREroAlYRERExlMKIiIiIGMqvw0hQUBBPPvkkQUFBRpdiGH//Hfj75wf9DvT5/fvzg34H3vD5feICVhEREam9/PrMiIiIiBhPYUREREQMpTAiIiIihlIYEREREUP5dRh58803ad68OcHBwXTv3p01a9YYXZLH/PTTTwwdOpS4uDhMJhNffvml0SV51LRp07jqqqsIDw8nOjqaP/7xj+zcudPosjxmxowZXHbZZaVNjnr06MF3331ndFmGef755zGZTEycONHoUjzmqaeewmQylXskJiYaXZZHHT16lD//+c/Ur1+fkJAQOnXqxLp164wuy2OaN2/+m38HTCYTEyZM8HgtfhtG/vOf//DQQw/x5JNPsmHDBjp37sygQYNIS0szujSPyM7OpnPnzrz55ptGl2KIZcuWMWHCBFatWkVycjKFhYUMHDiQ7Oxso0vziCZNmvD888+zfv161q1bR79+/Rg2bBhbt241ujSPW7t2LW+//TaXXXaZ0aV4XIcOHUhJSSl9LF++3OiSPObMmTP06tWLwMBAvvvuO7Zt28bLL79MvXr1jC7NY9auXVvun39ycjIAt9xyi+eLcfipbt26OSZMmFD6s81mc8TFxTmmTZtmYFXGABwLFiwwugxDpaWlOQDHsmXLjC7FMPXq1XO89957RpfhUZmZmY7WrVs7kpOTHddee63jgQceMLokj3nyyScdnTt3NroMwzz66KOO3r17G12GV3nggQccLVu2dNjtdo8f2y/PjBQUFLB+/XoGDBhQ+prZbGbAgAGsXLnSwMrEKOnp6QBERUUZXInn2Ww2PvnkE7Kzs+nRo4fR5XjUhAkTGDJkSLn/F/iT3bt3ExcXR4sWLRgxYgSHDh0yuiSP+frrr+natSu33HIL0dHRdOnShXfffdfosgxTUFDAnDlzGDt2rFsnpK0svwwjJ0+exGazERMTU+71mJgYjh8/blBVYhS73c7EiRPp1asXHTt2NLocj9m8eTN16tQhKCiI8ePHs2DBAtq3b290WR7zySefsGHDBqZNm2Z0KYbo3r07H374IYsWLWLGjBns37+fa665hszMTKNL84h9+/YxY8YMWrduTVJSEvfccw/3338/s2bNMro0Q3z55ZecPXuW0aNHG3J8n5i1V6QmTZgwgS1btvjVeDlA27Zt2bhxI+np6Xz22WeMGjWKZcuW+UUgOXz4MA888ADJyckEBwcbXY4hBg8eXLp82WWX0b17d5o1a8b8+fO58847DazMM+x2O127duW5554DoEuXLmzZsoW33nqLUaNGGVyd573//vsMHjyYuLg4Q47vl2dGGjRogMViITU1tdzrqampNGrUyKCqxAj33Xcf//3vf/nxxx9p0qSJ0eV4lNVqpVWrVlx55ZVMmzaNzp078+qrrxpdlkesX7+etLQ0rrjiCgICAggICGDZsmW89tprBAQEYLPZjC7R4+rWrUubNm3Ys2eP0aV4RGxs7G+Cd7t27fxqqKrEwYMH+eGHHxg3bpxhNfhlGLFarVx55ZUsXry49DW73c7ixYv9bszcXzkcDu677z4WLFjAkiVLSEhIMLokw9ntdvLz840uwyP69+/P5s2b2bhxY+mja9eujBgxgo0bN2KxWIwu0eOysrLYu3cvsbGxRpfiEb169frN7fy7du2iWbNmBlVknJkzZxIdHc2QIUMMq8Fvh2keeughRo0aRdeuXenWrRuvvPIK2dnZjBkzxujSPCIrK6vcX0D79+9n48aNREVF0bRpUwMr84wJEyYwb948vvrqK8LDw0uvFYqMjCQkJMTg6mre5MmTGTx4ME2bNiUzM5N58+axdOlSkpKSjC7NI8LDw39zfVBYWBj169f3m+uGHn74YYYOHUqzZs04duwYTz75JBaLheHDhxtdmkc8+OCD9OzZk+eee45bb72VNWvW8M477/DOO+8YXZpH2e12Zs6cyahRowgIMDASePz+HS/y+uuvO5o2beqwWq2Obt26OVatWmV0SR7z448/OoDfPEaNGmV0aR5xoc8OOGbOnGl0aR4xduxYR7NmzRxWq9XRsGFDR//+/R3ff/+90WUZyt9u7b3tttscsbGxDqvV6mjcuLHjtttuc+zZs8fosjzqm2++cXTs2NERFBTkSExMdLzzzjtGl+RxSUlJDsCxc+dOQ+swORwOhzExSERERMRPrxkRERER76EwIiIiIoZSGBERERFDKYyIiIiIoRRGRERExFAKIyIiImIohRERERExlMKIiIiIGEphRERERAylMCIiIiKGUhgRERERQymMiIiIiKH+PwtHaGkg4gIyAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T5 with Sir's code"
      ],
      "metadata": {
        "id": "mURz14cPMK86"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMzfFbCL3Fzx"
      },
      "outputs": [],
      "source": [
        "MAX_INPUT_LENGTH = 1024\n",
        "MIN_TARGET_LENGTH = 5\n",
        "MAX_TARGET_LENGTH = 128\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 0.002\n",
        "MAX_EPOCHS = 10\n",
        "MODEL_CHECKPOINT = \"t5-small\" # Name of Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DPT6-GE3F17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "81a57efa95124a6daacb36bd4fe92252",
            "a4a92ada78554a4ab06cf25350c39cf9",
            "323337bd0f704081bd592b32bd1824ef",
            "d6686c3f47914b23bb3acc8cc14471ba",
            "6cef58a813fb4620b17a68bd53a094fe",
            "20e5cd3fd9914e7fa06e5216e42a6d62",
            "eaf8406d929f4676b3f62ed4b1814aa5",
            "e951a926110a46af838a52ee502221b7",
            "eb0feeef614e429395d912c353a5ed22",
            "084817913fdf4cfc94bc50f60cca81e2",
            "6790bab457094e89a4e0fa7f58388ad6",
            "8ede49250f1e4606acd6e086d8988cc5",
            "8d728745d27049a7bcf406592b74d5a4",
            "6f6a00a781354969b9493ab172da645e",
            "af06185bb460494d8faed3b719f8af74",
            "87a037e769594c7e9b8deb88b3b7e6e6",
            "553b285cef0b4fb7b1d8c12e82c4c48a",
            "bac1a40c658e4728b774c1cb7a0758b4",
            "0a0df6b3ff1040a782312639508c4e65",
            "083530e04d954865a98166b2e829f583",
            "4c2ca2dc11a2442e94b8161d762c0b36",
            "0d65d981545a4537b411902683f90ebb",
            "5227e3d4eb5645c3b9606368739db7b1",
            "4d883570a5b34fd0a5746d496135597d",
            "417f1b1409ab49538045114b1a98e66b",
            "518cfb1289f94a6485033122b0a791e7",
            "ad820011ffff4d14ab1e3bd068c3e860",
            "3ecb10e2d66d43b9891666f9dea0ae53",
            "3b1e2a2f44ae4345a52c9f7f16d584a3",
            "26eef67c5d7a4f5b93750f0e5342a7c1",
            "74bad1b8610947af87bd16df5bebeae7",
            "d39c48c3fe5a4d1d9a8becda8c921c80",
            "4315997378244fdd919efbea51b136a0"
          ]
        },
        "outputId": "7bb8d1c0-62b5-4ad2-e64c-28096decd400"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81a57efa95124a6daacb36bd4fe92252"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ede49250f1e4606acd6e086d8988cc5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5227e3d4eb5645c3b9606368739db7b1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfBy3oBI3F3w"
      },
      "outputs": [],
      "source": [
        "if MODEL_CHECKPOINT in [\"t5-small\", \"t5-base\"]:\n",
        "  prefix = \"summarize: \"\n",
        "else:\n",
        "  prefix = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50Uft7As3F6d"
      },
      "outputs": [],
      "source": [
        "# #Preprocessing\n",
        "# def preprocess_function(examples):\n",
        "#   inputs = [prefix + doc for doc in examples[\"content\"]]\n",
        "#   model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH,truncation=True)\n",
        "#   # Setup the tokenizer for targets\n",
        "#   with tokenizer.as_target_tokenizer():\n",
        "#     labels = tokenizer(\n",
        "#     examples[\"title\"], max_length=MAX_TARGET_LENGTH, truncation=True\n",
        "#     )\n",
        "#   model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "#   return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F67DWlZ_CGQr",
        "outputId": "33e77968-59df-4f04-fb4e-756da9e54652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 817 entries, 0 to 823\n",
            "Data columns (total 6 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   keyword     817 non-null    object\n",
            " 1   url         817 non-null    object\n",
            " 2   title       817 non-null    object\n",
            " 3   content     817 non-null    object\n",
            " 4   urlToImage  770 non-null    object\n",
            " 5   author      732 non-null    object\n",
            "dtypes: object(6)\n",
            "memory usage: 44.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(row):\n",
        "    if 'content' in row and isinstance(row['content'], str):\n",
        "        document = row['content']\n",
        "        if MODEL_CHECKPOINT in [\"t5-small\", \"t5-base\"]:\n",
        "            document = \"summarize: \" + document\n",
        "        model_inputs = tokenizer(document, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "        if 'summary' in row and isinstance(row['summary'], str):\n",
        "            with tokenizer.as_target_tokenizer():\n",
        "                labels = tokenizer(row['summary'], max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "        return model_inputs\n",
        "    else:\n",
        "        return {}\n"
      ],
      "metadata": {
        "id": "ODzd79wN9tjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess_function(row):\n",
        "#     # Check if the 'content' column exists and is a non-empty string\n",
        "#     if 'content' in row and isinstance(row['content'], str):\n",
        "#         document = row['content']\n",
        "#         # Prefix if needed\n",
        "#         if MODEL_CHECKPOINT in [\"t5-small\", \"t5-base\"]:\n",
        "#             document = \"summarize: \" + document\n",
        "#         # Tokenize the document\n",
        "#         model_inputs = tokenizer(document, max_length=4000, truncation=True)\n",
        "#         # Setup the tokenizer for targets (if you have a 'summary' column)\n",
        "#         if 'summary' in row and isinstance(row['summary'], str):\n",
        "#             with tokenizer.as_target_tokenizer():\n",
        "#                 labels = tokenizer(row['summary'], max_length=4000, truncation=True)\n",
        "#             model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "#         return model_inputs\n",
        "#     else:\n",
        "#         return {}\n"
      ],
      "metadata": {
        "id": "JGC8hbHXtLuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8_rizYw3ku-"
      },
      "outputs": [],
      "source": [
        "tokenized_train = train.apply(preprocess_function, axis=1)\n",
        "tokenized_test = test.apply(preprocess_function, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YFgkaF13kyE"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the model name\n",
        "model_name = MODEL_CHECKPOINT.split(\"/\")[-1]\n",
        "\n",
        "# Define training arguments\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    f\"{model_name}-finetuned\",\n",
        "    evaluation_strategy=\"steps\",  # Set to \"steps\" for training without evaluation\n",
        "    learning_rate=0.0001,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=10,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True\n",
        ")"
      ],
      "metadata": {
        "id": "YBNw3LUI6-lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate\n",
        "accelerate.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "a9CsLZkN7Bzp",
        "outputId": "2ae7a5d7-8b85-41bb-c019-4cb8037d21a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.24.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input_shape as a tuple with two values\n",
        "input_shape = (8, seq_length)\n"
      ],
      "metadata": {
        "id": "XgIIxD7PHCwT",
        "outputId": "dd58bcfd-f75c-4a20-bfd4-ae0f5a3456ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-151-bfe7f400da1a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Define input_shape as a tuple with two values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'seq_length' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Seq2SeqTrainer and train the model\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model.to(device),\n",
        "    args,\n",
        "    train_dataset=tokenized_train,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "YKKjs3aS60pq",
        "outputId": "7829b3b5-79f4-4dcf-ee1e-0ebbac079305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-149-14a88d043d32>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1592\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1891\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1892\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1894\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2776\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2799\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2800\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2801\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2802\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2803\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1707\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m             \u001b[0;31m# Convert encoder inputs in embeddings if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1709\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1710\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1021\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;31m# required mask seq length can be calculated via length of past\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "LMK4c2nZpr7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new_content = \"New news article content goes here\"\n",
        "\n",
        "# # Generate a summary (Y)\n",
        "# generated_summary = model.generate_summary(new_content)\n",
        "\n",
        "# # Find the original URL associated with the content\n",
        "# original_url = df[df['content'] == new_content]['url'].values[0]\n",
        "\n",
        "# # Display the summary and URL\n",
        "# print(\"Generated Summary:\", generated_summary)\n",
        "# print(\"Original URL:\", original_url)\n"
      ],
      "metadata": {
        "id": "1ZIa8smwmFRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_row = train_data.sample(n=1)"
      ],
      "metadata": {
        "id": "sIc6eY_Gmkz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# later"
      ],
      "metadata": {
        "id": "oG1-_KJZ6lUb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUfBRis73k01"
      },
      "outputs": [],
      "source": [
        "# metric = load_metric(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the compute_metrics function\n",
        "# def compute_metrics(eval_pred):\n",
        "#     predictions, labels = eval_pred\n",
        "#     preds = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
        "#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "#     # Replace -100 in the labels as we can't decode them.\n",
        "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "#     # Rouge expects a newline after each sentence\n",
        "#     decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "#     decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "#     result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "#     # Extract a few results\n",
        "#     result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "#     # Add mean generated length\n",
        "#     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "#     result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "#     return {k: round(v, 4) for k, v in result.items()}\n"
      ],
      "metadata": {
        "id": "F-uXX46I4mx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHd-OeUZ3k4b"
      },
      "outputs": [],
      "source": [
        "# def compute_metrics(eval_pred, df, generated_col, reference_col):\n",
        "#     predictions, labels = eval_pred\n",
        "#     preds = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
        "#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "#     # Replace -100 in the labels as we can't decode them.\n",
        "#     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "#     # Rouge expects a newline after each sentence\n",
        "#     decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "#     decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "#     references = train_data[reference_col].tolist()\n",
        "\n",
        "#     result = metric.compute(predictions=decoded_preds, references=references, use_stemmer=True)\n",
        "\n",
        "#     # Extract a few results\n",
        "#     result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "#     # Add mean generated length\n",
        "#     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "#     result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "#     return {k: round(v, 4) for k, v in result.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w97xLMP139uk",
        "outputId": "1bf6aaab-a8d9-4738-d715-bf1283f9fe0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available and being used\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"GPU is available and being used\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "  print(\"GPU is not available, using CPU instead\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-toGiCh39x7"
      },
      "outputs": [],
      "source": [
        "model_name = MODEL_CHECKPOINT.split(\"/\")[-1]\n",
        "#model = model_name.to(device)\n",
        "args = Seq2SeqTrainingArguments(\n",
        "  f\"{model_name}-finetuned\",\n",
        "  evaluation_strategy = \"epoch\",\n",
        "  learning_rate=0.001,\n",
        "  per_device_train_batch_size=32,\n",
        "  per_device_eval_batch_size=32,\n",
        "  weight_decay=0.01,\n",
        "  save_total_limit=3,\n",
        "  num_train_epochs=10,\n",
        "  predict_with_generate=True,\n",
        "  fp16=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "m7cLx0dY3-Wl",
        "outputId": "53a4ebd6-7965-4407-c7c2-fed8b4699f4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.24.0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "import accelerate\n",
        "accelerate.__version__"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD0CiiAkvaFZ",
        "outputId": "277ecd32-a03e-46e9-f6dc-d79d07d9a060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "571"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4bYOP0V3-ZA"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "  model.to(device),\n",
        "  args,\n",
        "  train_dataset=tokenized_train,\n",
        "  eval_dataset=tokenized_test,\n",
        "  data_collator=data_collator,\n",
        "  tokenizer=tokenizer,\n",
        "  compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "b6EaOXum3-ch",
        "outputId": "9a890abd-f635-4fd1-82f6-c6b4e38275b5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 222",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-115-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1591\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1592\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1870\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1871\u001b[0m                 \u001b[0mtotal_batched_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrng_to_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 222"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx0B3mbY4QIB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "outputId": "f68355e7-cd9e-4aff-ba04-779af7fd1344"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3802\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-c8cca0812b07>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_seq2seq.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     def prediction_step(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3142\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   3143\u001b[0m             \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric_key_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3243\u001b[0m         \u001b[0mobserved_num_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3244\u001b[0m         \u001b[0;31m# Main evaluation loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3245\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3246\u001b[0m             \u001b[0;31m# Update the observed num examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3247\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;31m# We iterate one batch ahead to check when we are at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0mcurrent_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1090\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3804\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3805\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3806\u001b[0m                 \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 0"
          ]
        }
      ],
      "source": [
        "predict_results = trainer.predict(tokenized_test,max_length=128, num_beams=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Y1r8hhq4QKI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "da629bcc-a636-45bc-bd6d-9c7759d0847d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-80807fa7f58d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_with_generate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predict_results' is not defined"
          ]
        }
      ],
      "source": [
        "if args.predict_with_generate:\n",
        "\n",
        "  preds = np.where(predict_results.predictions != -100, predict_results.predictions, tokenizer.pad_token_id)\n",
        "  predictions = tokenizer.batch_decode(preds, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "  predictions = [pred.strip() for pred in predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZY4Mri54QMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d5cb18-7e43-4717-b778-256079c74d15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Three men accused of involvement in the murder of a Scottish woman in Mauritius have returned to the crime scene as part of the police investigation.',\n",
              " 'Covering costs of sending children back to school has left almost a fifth of Northern Ireland parents cutting their spending on food, a survey has found.']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "test['summary'][:2]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a DataFrame named train_data\n",
        "eval_predictions = trainer.predict(tokenized_test, max_length=128, num_beams=3)\n",
        "metrics = compute_metrics(eval_predictions, train_data, 'content', 'summary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "J0N52sjwtsJi",
        "outputId": "c987de1e-5cba-4887-ed70-adaa4be637b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-24ee8cbc1b59>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Assuming you have a DataFrame named train_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meval_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'content'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'summary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOKuTlIr4QOx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "e393aadc-6597-49ef-b437-aa71c92f000d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-119-1ed3eab7dca2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
          ]
        }
      ],
      "source": [
        "predictions[:2]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vwdtnDYi6oaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# t5 = \"t5-small\"  # You can choose a different T5 model if needed\n",
        "# tokenizer = T5Tokenizer.from_pretrained(t5)\n",
        "# model = T5ForConditionalGeneration.from_pretrained(t5)\n",
        "\n",
        "# # Create an empty list to store the generated summaries\n",
        "# summarizd = []\n",
        "# summarizd=test\n",
        "# # Iterate through the articles in your dataset\n",
        "# for index, row in train.iterrows():\n",
        "#     news_content = row['content']\n",
        "\n",
        "#     # Preprocess and tokenize the news article\n",
        "#     inputs = tokenizer.encode(\"summarize: \" + news_content, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "\n",
        "#     # Perform inference with the T5 model\n",
        "#     with torch.no_grad():\n",
        "#         summary_ids = model.generate(\n",
        "#             inputs,\n",
        "#             max_length=150,  # You can adjust this as needed\n",
        "#             min_length=40,   # You can adjust this as needed\n",
        "#             length_penalty=2.0,  # You can adjust this as needed\n",
        "#             num_beams=4,  # You can adjust this as needed\n",
        "#             early_stopping=True\n",
        "#         )\n",
        "\n",
        "#     # Decode the summary from the generated IDs\n",
        "#     summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "#     # Append the generated summary to the list\n",
        "#     summarizd.append(summary)\n",
        "\n",
        "# # Now, your 'summarizd' list contains T5-generated summaries\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "0BS7q_2k7Fej",
        "outputId": "4681c248-c6c5-4ec8-9fdd-8bd371cc370c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m842.4 kB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.3 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-a3bcc6b114d4>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mt5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"t5-small\"\u001b[0m  \u001b[0;31m# You can choose a different T5 model if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"_from_config\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1222\u001b[0;31m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backends\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     \u001b[0mfailed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchecks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1210\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}